{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy Timeseries Forecast\n",
    "## Predict occupancies for future hours from a known history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocess import read_and_preprocess_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import l2\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"challenge/data/device_activations_train.csv\"\n",
    "val_in_file = \"challenge/data/device_activations_val.csv\"\n",
    "test_in_file = \"challenge/data/device_activations_test.csv\"\n",
    "device_list=['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7']\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & preprocess data\n",
    "Load data from csv files, expects different files for training, validation and test (to be specified in the cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-07-01 04:00:00', '2016-07-01 05:00:00',\n",
      "               '2016-07-01 06:00:00', '2016-07-01 07:00:00',\n",
      "               '2016-07-01 08:00:00', '2016-07-01 09:00:00',\n",
      "               '2016-07-01 10:00:00', '2016-07-01 11:00:00',\n",
      "               '2016-07-01 12:00:00', '2016-07-01 13:00:00',\n",
      "               ...\n",
      "               '2016-08-12 00:00:00', '2016-08-12 01:00:00',\n",
      "               '2016-08-12 02:00:00', '2016-08-12 03:00:00',\n",
      "               '2016-08-12 04:00:00', '2016-08-12 05:00:00',\n",
      "               '2016-08-12 06:00:00', '2016-08-12 07:00:00',\n",
      "               '2016-08-12 08:00:00', '2016-08-12 09:00:00'],\n",
      "              dtype='datetime64[ns]', length=1014, freq='H')\n",
      "Hours in data:  1014\n",
      "[[ 4.          5.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.          6.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          0.28402367  0.02366864\n",
      "   0.          0.          0.1183432   0.        ]\n",
      " [ 4.          7.          0.          1.          0.          1.\n",
      "   0.          1.          0.          0.1183432   1.06508876  0.07100592\n",
      "   0.09467456  0.14201183  0.52071006  0.        ]\n",
      " [ 4.          8.          0.          1.          1.          1.\n",
      "   1.          1.          0.          0.16568047  1.11242604  0.1183432\n",
      "   0.21301775  0.61538462  0.63905325  0.        ]\n",
      " [ 4.          9.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.1183432   0.87573964  0.14201183\n",
      "   0.35502959  0.49704142  0.56804734  0.        ]\n",
      " [ 4.         10.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.          0.5443787   0.16568047\n",
      "   0.14201183  0.4260355   0.33136095  0.        ]\n",
      " [ 4.         11.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.04733728  0.56804734  0.04733728\n",
      "   0.28402367  0.33136095  0.4260355   0.        ]\n",
      " [ 4.         12.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.09467456  0.94674556  0.14201183\n",
      "   0.1183432   0.09467456  0.47337278  0.        ]\n",
      " [ 4.         13.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.16568047  1.13609467  0.33136095\n",
      "   0.30769231  0.4260355   0.49704142  0.        ]\n",
      " [ 4.         14.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.07100592  0.94674556  0.23668639\n",
      "   0.26035503  0.5443787   0.61538462  0.02366864]]\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0]\n",
      " [0 1 1 1 1 1 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]]\n",
      "File challenge/data/device_activations_train.csv has 1013 timesteps (hours) until now\n",
      "initial features shape:  (1013, 16)\n",
      "Full sequence length:  900\n",
      "Sequence 0 has start index 0 and end index 900\n",
      "(900, 16)\n",
      "Sequence 1 has start index 20 and end index 920\n",
      "(900, 16)\n",
      "Sequence 2 has start index 40 and end index 940\n",
      "(900, 16)\n",
      "Sequence 3 has start index 60 and end index 960\n",
      "(900, 16)\n",
      "Features sequences shape:  (4, 9, 100, 16)\n",
      "Labels sequences shape:  (4, 9, 100, 7)\n",
      "Feature batch shape:  (36, 100, 16)\n",
      "Label batch shape:  (36, 100, 7)\n"
     ]
    }
   ],
   "source": [
    "feature_batch, label_batch, device_list, mean_occupancies = read_and_preprocess_data(in_file, batch_size=BATCH_SIZE, device_list=device_list, sequence_start_shift=20)\n",
    "print(\"Feature batch shape: \", feature_batch.shape)\n",
    "print(\"Label batch shape: \", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean occupancy visualization per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.04395604 0.1487743  0.31445478 0.36517329 0.31107354 0.40236686\n",
      " 0.28064243 0.43956044 0.44632291 0.51394759 0.44294167 0.16906171\n",
      " 0.03381234 0.03043111 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.04057481 0.21301775 0.30431107 0.36855452 0.40912933 0.41251057\n",
      " 0.26373626 0.40912933 0.45984784 0.51394759 0.4057481  0.16568047\n",
      " 0.04057481 0.0202874  0.04057481 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07100592 0.34826712 0.44632291 0.44294167 0.34150465 0.3956044\n",
      " 0.38884193 0.48689772 0.51394759 0.54775993 0.44632291 0.29416737\n",
      " 0.06762468 0.00676247 0.         0.00338123 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05748098 0.30769231 0.49027895 0.5545224  0.52747253 0.44970414\n",
      " 0.45646661 0.52409129 0.55114117 0.534235   0.37531699 0.22316145\n",
      " 0.08453085 0.02704987 0.00676247 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00338123 0.\n",
      " 0.06086221 0.2874049  0.40912933 0.36517329 0.22992392 0.24344886\n",
      " 0.2671175  0.40912933 0.38546069 0.31783601 0.18258664 0.06424345\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00338123 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00338123 0.0202874  0.00676247 0.\n",
      " 0.         0.         0.         0.         0.         0.00338123\n",
      " 0.01690617 0.         0.         0.         0.00338123 0.\n",
      " 0.         0.         0.00338123 0.         0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_occupancy_per_hour = mean_occupancies.to_numpy().reshape([168, 7]).sum(axis=1) / 7\n",
    "labels = range(len(mean_occupancy_per_hour))\n",
    "print(mean_occupancy_per_hour)\n",
    "plt.bar(labels, mean_occupancy_per_hour, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-08-12 10:00:00', '2016-08-12 11:00:00',\n",
      "               '2016-08-12 12:00:00', '2016-08-12 13:00:00',\n",
      "               '2016-08-12 14:00:00', '2016-08-12 15:00:00',\n",
      "               '2016-08-12 16:00:00', '2016-08-12 17:00:00',\n",
      "               '2016-08-12 18:00:00', '2016-08-12 19:00:00',\n",
      "               ...\n",
      "               '2016-08-24 00:00:00', '2016-08-24 01:00:00',\n",
      "               '2016-08-24 02:00:00', '2016-08-24 03:00:00',\n",
      "               '2016-08-24 04:00:00', '2016-08-24 05:00:00',\n",
      "               '2016-08-24 06:00:00', '2016-08-24 07:00:00',\n",
      "               '2016-08-24 08:00:00', '2016-08-24 09:00:00'],\n",
      "              dtype='datetime64[ns]', length=288, freq='H')\n",
      "Hours in data:  288\n",
      "[[ 4.         11.          1.          1.          0.          1.\n",
      "   1.          1.          1.          0.91666667  0.83333333  0.\n",
      "   0.08333333  0.75        0.5         0.08333333]\n",
      " [ 4.         12.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.83333333  1.25        0.08333333\n",
      "   0.41666667  0.41666667  0.66666667  0.        ]\n",
      " [ 4.         13.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.91666667  1.5         0.16666667\n",
      "   0.08333333  0.83333333  0.91666667  0.08333333]\n",
      " [ 4.         14.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.83333333  0.75        0.25\n",
      "   0.41666667  0.66666667  0.91666667  0.25      ]\n",
      " [ 4.         15.          0.          1.          1.          0.\n",
      "   0.          1.          0.          0.          0.41666667  0.08333333\n",
      "   0.          0.          0.5         0.        ]\n",
      " [ 4.         16.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.08333333  0.          0.\n",
      "   0.          0.          0.16666667  0.        ]\n",
      " [ 4.         17.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.16666667  0.        ]\n",
      " [ 4.         18.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         19.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[[1 1 1 1 1 1 0]\n",
      " [1 1 1 1 1 1 0]\n",
      " [1 1 1 1 1 1 0]\n",
      " [0 1 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "File challenge/data/device_activations_val.csv has 287 timesteps (hours) until now\n",
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-08-24 10:00:00', '2016-08-24 11:00:00',\n",
      "               '2016-08-24 12:00:00', '2016-08-24 13:00:00',\n",
      "               '2016-08-24 14:00:00', '2016-08-24 15:00:00',\n",
      "               '2016-08-24 16:00:00', '2016-08-24 17:00:00',\n",
      "               '2016-08-24 18:00:00', '2016-08-24 19:00:00',\n",
      "               ...\n",
      "               '2016-08-31 08:00:00', '2016-08-31 09:00:00',\n",
      "               '2016-08-31 10:00:00', '2016-08-31 11:00:00',\n",
      "               '2016-08-31 12:00:00', '2016-08-31 13:00:00',\n",
      "               '2016-08-31 14:00:00', '2016-08-31 15:00:00',\n",
      "               '2016-08-31 16:00:00', '2016-08-31 17:00:00'],\n",
      "              dtype='datetime64[ns]', length=176, freq='H')\n",
      "Hours in data:  176\n",
      "[[ 2.         11.          1.          1.          1.          0.\n",
      "   0.          1.          0.          0.27272727  2.45454545  0.81818182\n",
      "   0.81818182  0.27272727  1.5         0.        ]\n",
      " [ 2.         12.          1.          1.          1.          0.\n",
      "   0.          1.          0.          0.81818182  1.90909091  0.95454545\n",
      "   0.27272727  0.13636364  1.36363636  0.        ]\n",
      " [ 2.         13.          1.          1.          1.          0.\n",
      "   0.          1.          0.          1.36363636  1.90909091  1.5\n",
      "   0.81818182  0.54545455  1.5         0.        ]\n",
      " [ 2.         14.          1.          1.          1.          0.\n",
      "   0.          1.          0.          1.22727273  2.72727273  0.95454545\n",
      "   0.40909091  0.13636364  1.36363636  0.        ]\n",
      " [ 2.         15.          1.          1.          0.          0.\n",
      "   0.          1.          0.          0.54545455  1.63636364  0.\n",
      "   0.54545455  0.81818182  1.5         0.        ]\n",
      " [ 2.         16.          0.          1.          0.          0.\n",
      "   0.          1.          0.          0.81818182  1.77272727  0.\n",
      "   0.54545455  0.          1.36363636  0.        ]\n",
      " [ 2.         17.          1.          0.          0.          0.\n",
      "   0.          1.          0.          0.13636364  0.          0.\n",
      "   0.40909091  0.          0.68181818  0.        ]\n",
      " [ 2.         18.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.         19.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[[1 1 1 0 0 1 0]\n",
      " [1 1 1 0 0 1 0]\n",
      " [1 1 1 0 0 1 0]\n",
      " [1 1 0 0 0 1 0]\n",
      " [0 1 0 0 0 1 0]\n",
      " [1 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]]\n",
      "File challenge/data/device_activations_test.csv has 175 timesteps (hours) until now\n"
     ]
    }
   ],
   "source": [
    "val_features, val_labels, _, _ = read_and_preprocess_data(val_in_file, batch_size=1, device_list=device_list)\n",
    "test_features, test_labels, _, _ = read_and_preprocess_data(test_in_file, batch_size=1, device_list=device_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ratio of positive to negative labels per device to adjust loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive to negative outputs per device:  [0.08333333 0.24305556 0.165      0.2375     0.23277778 0.33444444\n",
      " 0.055     ]\n"
     ]
    }
   ],
   "source": [
    "def calc_ratio_positive_outputs_per_device(labels):\n",
    "    ratio_per_device = np.sum(labels, axis=0) / labels.shape[0]\n",
    "    print(\"Ratio of positive to negative outputs per device: \", ratio_per_device)\n",
    "    return np.array(ratio_per_device)\n",
    "ratio_positive_outputs_per_device = calc_ratio_positive_outputs_per_device(label_batch.reshape([-1, label_batch.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted loss: Scales up loss for positive labels per-device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own weighted loss to combat label imbalance\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    out = -(y_true * K.log(y_pred + 1e-5) / ratio_positive_outputs_per_device + (1.0 - y_true) * K.log(1.0 - y_pred + 1e-5))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \"\"\"\n",
    "    Creates an LSTM-based model given the parameters: 'lr', 'do', 'reg', 'lstm_units', 'n_outputs', 'n_features', 'use_weighted_loss'.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['lstm_units'], batch_input_shape=(params['batch_size'], None, params['n_features']), return_sequences=True, stateful=True, kernel_regularizer=l2(params['reg'])))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(params['n_outputs'], activation='sigmoid'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=params['lr'])\n",
    "    model.compile(loss=weighted_loss if params['use_weighted_loss'] else 'binary_crossentropy', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_24h(model, features, labels):\n",
    "    \"\"\"\n",
    "    Predicts a model's output for given features and the following 24 hours, reusing predictions as inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = np.squeeze(model.predict(np.expand_dims(features, 0), batch_size=1))  # (n_timesteps, n_outputs)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    last_features = np.squeeze(features)[-1]\n",
    "    last_predictions = tmp_prediction = predictions[-1]\n",
    "    tmp_features = np.array(last_features)\n",
    "    tmp_mean_occupancies = [mean_occupancies.loc[(tmp_features[0] * 24 + tmp_features[1], 'device_' + str(j + 1)), \n",
    "                                             'mean_occupancy'] for j in range(len(device_list))]\n",
    "    for i in range(24):\n",
    "\n",
    "        tmp_features = np.concatenate([tmp_features[:2], np.round(np.squeeze(tmp_prediction)), tmp_mean_occupancies])\n",
    "        \n",
    "        # Increment time features\n",
    "        if tmp_features[1] == 23:\n",
    "            tmp_features[0] = (tmp_features[0] + 1) % 7\n",
    "        tmp_features[1] = (tmp_features[1] + 1) % 24\n",
    "        \n",
    "        tmp_mean_occupancies = [mean_occupancies.loc[(tmp_features[0] * 24 + tmp_features[1], 'device_' + str(j + 1)), \n",
    "                                             'mean_occupancy'] for j in range(len(device_list))]\n",
    "        \n",
    "        tmp_prediction = np.round(model.predict(np.reshape(tmp_features, [1, 1, len(tmp_features)])))\n",
    "        \n",
    "        all_predictions += [tmp_prediction]\n",
    "\n",
    "    return np.concatenate(all_predictions)\n",
    "\n",
    "def calc_accuracy(model, params, test_X, test_Y):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of a model on a test (or validation) set. Generates sub-sequences of the test set to test on.\n",
    "    The model gets i hours as inputs before having to predict the next 24 hours (i in range(48, n - 24) with \n",
    "    n = hours in test set). This gives more \"effective\" test set size than splitting the test set in non-overlapping\n",
    "    sequences. Depending on the real world scenario, the assumption of at least 48 hours of input might be relaxed/\n",
    "    strictened.\n",
    "    \"\"\"\n",
    "    # Hack around Keras batch size restriction (to have same for training/test)\n",
    "    model.save('tmp_model.h5')\n",
    "    test_params = dict(params)\n",
    "    test_params['batch_size'] = 1\n",
    "    test_model = create_model(test_params)\n",
    "    test_model.load_weights('tmp_model.h5')\n",
    "    os.remove('tmp_model.h5')\n",
    "    n = test_X.shape[1]\n",
    "    acc_accumulated = 0.0\n",
    "    for i in range(48, n - 24):\n",
    "        predictions = np.squeeze(predict_24h(test_model, test_X[0, :i], test_Y[0, :i]))\n",
    "        true_labels = test_Y[0, i:i+24]\n",
    "        \n",
    "        acc = np.sum(np.round(predictions) == true_labels) / predictions.size\n",
    "        acc_accumulated += acc\n",
    "    \n",
    "    return acc_accumulated / (n-72) # n-72 as we leave out 48 in the beginning and 24 at the end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model_with_params(params, train_X, train_Y):\n",
    "    model = create_model(params)\n",
    "    history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch_size'], verbose=1, shuffle=False)\n",
    "    return model\n",
    "\n",
    "def eval_model_params(params, train_X, train_Y, val_X, val_Y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model on a given validation set by calculating its accuracy on a 24h prediction basis as \n",
    "    described in calc_accuracy().\n",
    "    \"\"\"\n",
    "    model = train_model_with_params(params, train_X, train_Y)\n",
    "    val_acc = calc_accuracy(model, params, val_X, val_Y)\n",
    "    return model, val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter (Grid) Search\n",
    "Usually I use the BayesianOptimization framework to efficiently calculate and evaluate hyperparameter spaces as GS\n",
    "gets out of hand quickly in terms of computational efficiency. In this case I simply ran out of time for that,\n",
    "so GS had to suffice. RS would have been an option, but I'm not too much of a fan and the training times in this \n",
    "project were \"ok\", so GS was my choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 1.2599\n",
      "Epoch 2/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 1.1767\n",
      "Epoch 3/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.1196\n",
      "Epoch 4/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.0705\n",
      "Epoch 5/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.0260\n",
      "Epoch 6/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.9823\n",
      "Epoch 7/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.9375\n",
      "Epoch 8/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.8930\n",
      "Epoch 9/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.8483\n",
      "Epoch 10/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.8036\n",
      "Epoch 11/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.7618\n",
      "Epoch 12/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.7228\n",
      "Epoch 13/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.6875\n",
      "Epoch 14/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.6560\n",
      "Epoch 15/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6286\n",
      "Epoch 16/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6044\n",
      "Epoch 17/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5834\n",
      "Epoch 18/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5658\n",
      "Epoch 19/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5508\n",
      "Epoch 20/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5382\n",
      "Epoch 21/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5266\n",
      "Epoch 22/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5165\n",
      "Epoch 23/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5080\n",
      "Epoch 24/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5006\n",
      "Epoch 25/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4942\n",
      "Epoch 26/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4887\n",
      "Epoch 27/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4838\n",
      "Epoch 28/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4793\n",
      "Epoch 29/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4751\n",
      "Epoch 30/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4713\n",
      "Epoch 31/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4677\n",
      "Epoch 32/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4642\n",
      "Epoch 33/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4609\n",
      "Epoch 34/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4577\n",
      "Epoch 35/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4545\n",
      "Epoch 36/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4513\n",
      "Epoch 37/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4485\n",
      "Epoch 38/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4455\n",
      "Epoch 39/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4426\n",
      "Epoch 40/250\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.4396\n",
      "Epoch 41/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4372\n",
      "Epoch 42/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4341\n",
      "Epoch 43/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4326\n",
      "Epoch 44/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4288\n",
      "Epoch 45/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4268\n",
      "Epoch 46/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4260\n",
      "Epoch 47/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4242\n",
      "Epoch 48/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4194\n",
      "Epoch 49/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4173\n",
      "Epoch 50/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4186\n",
      "Epoch 51/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.4117\n",
      "Epoch 52/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4104\n",
      "Epoch 53/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4130\n",
      "Epoch 54/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4054\n",
      "Epoch 55/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4021\n",
      "Epoch 56/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3978\n",
      "Epoch 57/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3947\n",
      "Epoch 58/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3972\n",
      "Epoch 59/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3948\n",
      "Epoch 60/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.3947\n",
      "Epoch 61/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3894\n",
      "Epoch 62/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3855\n",
      "Epoch 63/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3858\n",
      "Epoch 64/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3805\n",
      "Epoch 65/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.3770\n",
      "Epoch 66/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3733\n",
      "Epoch 67/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3683\n",
      "Epoch 68/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3662\n",
      "Epoch 69/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3642\n",
      "Epoch 70/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3618\n",
      "Epoch 71/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3614\n",
      "Epoch 72/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3598\n",
      "Epoch 73/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3633\n",
      "Epoch 74/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3586\n",
      "Epoch 75/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3581\n",
      "Epoch 76/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3522\n",
      "Epoch 77/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.3506\n",
      "Epoch 78/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3507\n",
      "Epoch 79/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3618\n",
      "Epoch 80/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3536\n",
      "Epoch 81/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3430\n",
      "Epoch 82/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3397\n",
      "Epoch 83/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3401\n",
      "Epoch 84/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3473\n",
      "Epoch 85/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.3419\n",
      "Epoch 86/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3337\n",
      "Epoch 87/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3309\n",
      "Epoch 88/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3248\n",
      "Epoch 89/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3285\n",
      "Epoch 90/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3296\n",
      "Epoch 91/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3290\n",
      "Epoch 92/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3268\n",
      "Epoch 93/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3232\n",
      "Epoch 94/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3176\n",
      "Epoch 95/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3177\n",
      "Epoch 96/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3196\n",
      "Epoch 97/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3135\n",
      "Epoch 98/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3093\n",
      "Epoch 99/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3110\n",
      "Epoch 100/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3145\n",
      "Epoch 101/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3130\n",
      "Epoch 102/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3063\n",
      "Epoch 103/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3034\n",
      "Epoch 104/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3052\n",
      "Epoch 105/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3258\n",
      "Epoch 106/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3198\n",
      "Epoch 107/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3072\n",
      "Epoch 108/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2992\n",
      "Epoch 109/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3001\n",
      "Epoch 110/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2946\n",
      "Epoch 111/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2913\n",
      "Epoch 112/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2910\n",
      "Epoch 113/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2874\n",
      "Epoch 114/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2874\n",
      "Epoch 115/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2851\n",
      "Epoch 116/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2837\n",
      "Epoch 117/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2830\n",
      "Epoch 118/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2806\n",
      "Epoch 119/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2808\n",
      "Epoch 120/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2834\n",
      "Epoch 121/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2797\n",
      "Epoch 122/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2838\n",
      "Epoch 123/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2794\n",
      "Epoch 124/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2794\n",
      "Epoch 125/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2812\n",
      "Epoch 126/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2761\n",
      "Epoch 127/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2743\n",
      "Epoch 128/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2880\n",
      "Epoch 129/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2802\n",
      "Epoch 130/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2763\n",
      "Epoch 131/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2763\n",
      "Epoch 132/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2691\n",
      "Epoch 133/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2653\n",
      "Epoch 134/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2646\n",
      "Epoch 135/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2675\n",
      "Epoch 136/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2635\n",
      "Epoch 137/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2656\n",
      "Epoch 138/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2615\n",
      "Epoch 139/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.2591\n",
      "Epoch 140/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2579\n",
      "Epoch 141/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2576\n",
      "Epoch 142/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2573\n",
      "Epoch 143/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2664\n",
      "Epoch 144/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3234\n",
      "Epoch 145/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2898\n",
      "Epoch 146/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3285\n",
      "Epoch 147/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3009\n",
      "Epoch 148/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2864\n",
      "Epoch 149/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2812\n",
      "Epoch 150/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2790\n",
      "Epoch 151/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2773\n",
      "Epoch 152/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2749\n",
      "Epoch 153/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2710\n",
      "Epoch 154/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2674\n",
      "Epoch 155/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2922\n",
      "Epoch 156/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2818\n",
      "Epoch 157/250\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.2632\n",
      "Epoch 158/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2543\n",
      "Epoch 159/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2512\n",
      "Epoch 160/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2485\n",
      "Epoch 161/250\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.2458\n",
      "Epoch 162/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2462\n",
      "Epoch 163/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2434\n",
      "Epoch 164/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2422\n",
      "Epoch 165/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2410\n",
      "Epoch 166/250\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 0.2406\n",
      "Epoch 167/250\n",
      "36/36 [==============================] - 1s 24ms/step - loss: 0.2404\n",
      "Epoch 168/250\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 0.2382\n",
      "Epoch 169/250\n",
      "36/36 [==============================] - 1s 23ms/step - loss: 0.2370\n",
      "Epoch 170/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2381\n",
      "Epoch 171/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2350\n",
      "Epoch 172/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2343\n",
      "Epoch 173/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2334\n",
      "Epoch 174/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2333\n",
      "Epoch 175/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2333\n",
      "Epoch 176/250\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.2319\n",
      "Epoch 177/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2303\n",
      "Epoch 178/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2314\n",
      "Epoch 179/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2289\n",
      "Epoch 180/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2299\n",
      "Epoch 181/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2286\n",
      "Epoch 182/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2310\n",
      "Epoch 183/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2292\n",
      "Epoch 184/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2269\n",
      "Epoch 185/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2262\n",
      "Epoch 186/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2287\n",
      "Epoch 187/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.2266\n",
      "Epoch 188/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2237\n",
      "Epoch 189/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2257\n",
      "Epoch 190/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2225\n",
      "Epoch 191/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2218\n",
      "Epoch 192/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2238\n",
      "Epoch 193/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2208\n",
      "Epoch 194/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2204\n",
      "Epoch 195/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2180\n",
      "Epoch 196/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2183\n",
      "Epoch 197/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2208\n",
      "Epoch 198/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2264\n",
      "Epoch 199/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2632\n",
      "Epoch 200/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2560\n",
      "Epoch 201/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2399\n",
      "Epoch 202/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2276\n",
      "Epoch 203/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2223\n",
      "Epoch 204/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2214\n",
      "Epoch 205/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2190\n",
      "Epoch 206/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2168\n",
      "Epoch 207/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2152\n",
      "Epoch 208/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2143\n",
      "Epoch 209/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2133\n",
      "Epoch 210/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2128\n",
      "Epoch 211/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2118\n",
      "Epoch 212/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2120\n",
      "Epoch 213/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2133\n",
      "Epoch 214/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2101\n",
      "Epoch 215/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2109\n",
      "Epoch 216/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2092\n",
      "Epoch 217/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2085\n",
      "Epoch 218/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2079\n",
      "Epoch 219/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2068\n",
      "Epoch 220/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2069\n",
      "Epoch 221/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2054\n",
      "Epoch 222/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2057\n",
      "Epoch 223/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2041\n",
      "Epoch 224/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2046\n",
      "Epoch 225/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2029\n",
      "Epoch 226/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2030\n",
      "Epoch 227/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2020\n",
      "Epoch 228/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2016\n",
      "Epoch 229/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2015\n",
      "Epoch 230/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2009\n",
      "Epoch 231/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2029\n",
      "Epoch 232/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2025\n",
      "Epoch 233/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2079\n",
      "Epoch 234/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2065\n",
      "Epoch 235/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2058\n",
      "Epoch 236/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2349\n",
      "Epoch 237/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2593\n",
      "Epoch 238/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2311\n",
      "Epoch 239/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2215\n",
      "Epoch 240/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2109\n",
      "Epoch 241/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2036\n",
      "Epoch 242/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2013\n",
      "Epoch 243/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1975\n",
      "Epoch 244/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1947\n",
      "Epoch 245/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1939\n",
      "Epoch 246/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1935\n",
      "Epoch 247/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1924\n",
      "Epoch 248/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1917\n",
      "Epoch 249/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1915\n",
      "Epoch 250/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1910\n",
      "Tmp result:  0.7798449612403096 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 4, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Epoch 1/250\n",
      "36/36 [==============================] - 1s 31ms/step - loss: 1.2938\n",
      "Epoch 2/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2875\n",
      "Epoch 3/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2818\n",
      "Epoch 4/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2764\n",
      "Epoch 5/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2711\n",
      "Epoch 6/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2660\n",
      "Epoch 7/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2611\n",
      "Epoch 8/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.2564\n",
      "Epoch 9/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2517\n",
      "Epoch 10/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2471\n",
      "Epoch 11/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2425\n",
      "Epoch 12/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2380\n",
      "Epoch 13/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2335\n",
      "Epoch 14/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2290\n",
      "Epoch 15/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2245\n",
      "Epoch 16/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2200\n",
      "Epoch 17/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2155\n",
      "Epoch 18/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2110\n",
      "Epoch 19/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2064\n",
      "Epoch 20/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2017\n",
      "Epoch 21/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.1970\n",
      "Epoch 22/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.1921\n",
      "Epoch 23/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.1871\n",
      "Epoch 24/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.1820\n",
      "Epoch 25/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.1767\n",
      "Epoch 26/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.1714\n",
      "Epoch 27/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.1659\n",
      "Epoch 28/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.1603\n",
      "Epoch 29/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 1.1545\n",
      "Epoch 30/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 1.1487\n",
      "Epoch 31/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 1.1430\n",
      "Epoch 32/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.1375\n",
      "Epoch 33/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.1320\n",
      "Epoch 34/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.1265\n",
      "Epoch 35/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.1209\n",
      "Epoch 36/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.1153\n",
      "Epoch 37/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.1095\n",
      "Epoch 38/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 1.1036\n",
      "Epoch 39/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.0976\n",
      "Epoch 40/250\n",
      "36/36 [==============================] - 1s 22ms/step - loss: 1.0914\n",
      "Epoch 41/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 1.0851\n",
      "Epoch 42/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 1.0786\n",
      "Epoch 43/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.0720\n",
      "Epoch 44/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 1.0652\n",
      "Epoch 45/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 16ms/step - loss: 1.0583\n",
      "Epoch 46/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.0513\n",
      "Epoch 47/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.0442\n",
      "Epoch 48/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 1.0371\n",
      "Epoch 49/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.0299\n",
      "Epoch 50/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.0227\n",
      "Epoch 51/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.0156\n",
      "Epoch 52/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.0085\n",
      "Epoch 53/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 1.0014\n",
      "Epoch 54/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9944\n",
      "Epoch 55/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9875\n",
      "Epoch 56/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9806\n",
      "Epoch 57/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9739\n",
      "Epoch 58/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.9673\n",
      "Epoch 59/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.9607\n",
      "Epoch 60/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.9542\n",
      "Epoch 61/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.9478\n",
      "Epoch 62/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.9414\n",
      "Epoch 63/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.9351\n",
      "Epoch 64/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9289\n",
      "Epoch 65/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.9227\n",
      "Epoch 66/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9166\n",
      "Epoch 67/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9105\n",
      "Epoch 68/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.9045\n",
      "Epoch 69/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8986\n",
      "Epoch 70/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8927\n",
      "Epoch 71/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8869\n",
      "Epoch 72/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.8811\n",
      "Epoch 73/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8754\n",
      "Epoch 74/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8698\n",
      "Epoch 75/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8642\n",
      "Epoch 76/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8587\n",
      "Epoch 77/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8532\n",
      "Epoch 78/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8478\n",
      "Epoch 79/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8425\n",
      "Epoch 80/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8372\n",
      "Epoch 81/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8319\n",
      "Epoch 82/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8266\n",
      "Epoch 83/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8214\n",
      "Epoch 84/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8161\n",
      "Epoch 85/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8109\n",
      "Epoch 86/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8057\n",
      "Epoch 87/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8005\n",
      "Epoch 88/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7952\n",
      "Epoch 89/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7900\n",
      "Epoch 90/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7848\n",
      "Epoch 91/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7796\n",
      "Epoch 92/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7744\n",
      "Epoch 93/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7691\n",
      "Epoch 94/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7639\n",
      "Epoch 95/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7586\n",
      "Epoch 96/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7534\n",
      "Epoch 97/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.7483\n",
      "Epoch 98/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7432\n",
      "Epoch 99/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7382\n",
      "Epoch 100/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7333\n",
      "Epoch 101/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7285\n",
      "Epoch 102/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7237\n",
      "Epoch 103/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7191\n",
      "Epoch 104/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7145\n",
      "Epoch 105/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7100\n",
      "Epoch 106/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7055\n",
      "Epoch 107/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7012\n",
      "Epoch 108/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.6969\n",
      "Epoch 109/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6926\n",
      "Epoch 110/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.6885\n",
      "Epoch 111/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6844\n",
      "Epoch 112/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6803\n",
      "Epoch 113/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6763\n",
      "Epoch 114/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6724\n",
      "Epoch 115/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6685\n",
      "Epoch 116/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6647\n",
      "Epoch 117/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6609\n",
      "Epoch 118/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6572\n",
      "Epoch 119/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6536\n",
      "Epoch 120/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6499\n",
      "Epoch 121/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6464\n",
      "Epoch 122/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6429\n",
      "Epoch 123/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6394\n",
      "Epoch 124/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6360\n",
      "Epoch 125/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6327\n",
      "Epoch 126/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6293\n",
      "Epoch 127/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.6260\n",
      "Epoch 128/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6228\n",
      "Epoch 129/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6195\n",
      "Epoch 130/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6164\n",
      "Epoch 131/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6132\n",
      "Epoch 132/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.6102\n",
      "Epoch 133/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.6071\n",
      "Epoch 134/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.6042\n",
      "Epoch 135/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.6012\n",
      "Epoch 136/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5984\n",
      "Epoch 137/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5955\n",
      "Epoch 138/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5928\n",
      "Epoch 139/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5900\n",
      "Epoch 140/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5873\n",
      "Epoch 141/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.5846\n",
      "Epoch 142/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5820\n",
      "Epoch 143/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5794\n",
      "Epoch 144/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5769\n",
      "Epoch 145/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5744\n",
      "Epoch 146/250\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.5720\n",
      "Epoch 147/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5696\n",
      "Epoch 148/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5672\n",
      "Epoch 149/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5648\n",
      "Epoch 150/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5625\n",
      "Epoch 151/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5603\n",
      "Epoch 152/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5580\n",
      "Epoch 153/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5558\n",
      "Epoch 154/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5537\n",
      "Epoch 155/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5516\n",
      "Epoch 156/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5495\n",
      "Epoch 157/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5474\n",
      "Epoch 158/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5454\n",
      "Epoch 159/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5434\n",
      "Epoch 160/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5415\n",
      "Epoch 161/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5396\n",
      "Epoch 162/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5377\n",
      "Epoch 163/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5358\n",
      "Epoch 164/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5340\n",
      "Epoch 165/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5323\n",
      "Epoch 166/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5305\n",
      "Epoch 167/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5288\n",
      "Epoch 168/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5271\n",
      "Epoch 169/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5254\n",
      "Epoch 170/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5238\n",
      "Epoch 171/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5222\n",
      "Epoch 172/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5206\n",
      "Epoch 173/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5191\n",
      "Epoch 174/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5176\n",
      "Epoch 175/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5161\n",
      "Epoch 176/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5146\n",
      "Epoch 177/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5132\n",
      "Epoch 178/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5118\n",
      "Epoch 179/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5105\n",
      "Epoch 180/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5091\n",
      "Epoch 181/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5078\n",
      "Epoch 182/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5065\n",
      "Epoch 183/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5052\n",
      "Epoch 184/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5040\n",
      "Epoch 185/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5027\n",
      "Epoch 186/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5015\n",
      "Epoch 187/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5003\n",
      "Epoch 188/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4992\n",
      "Epoch 189/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.4980\n",
      "Epoch 190/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4969\n",
      "Epoch 191/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4958\n",
      "Epoch 192/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4947\n",
      "Epoch 193/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4937\n",
      "Epoch 194/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4926\n",
      "Epoch 195/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4916\n",
      "Epoch 196/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4906\n",
      "Epoch 197/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4896\n",
      "Epoch 198/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4887\n",
      "Epoch 199/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4877\n",
      "Epoch 200/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4868\n",
      "Epoch 201/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4858\n",
      "Epoch 202/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4849\n",
      "Epoch 203/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.4840\n",
      "Epoch 204/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4831\n",
      "Epoch 205/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4822\n",
      "Epoch 206/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4813\n",
      "Epoch 207/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4803\n",
      "Epoch 208/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4794\n",
      "Epoch 209/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4785\n",
      "Epoch 210/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4776\n",
      "Epoch 211/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4767\n",
      "Epoch 212/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4758\n",
      "Epoch 213/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4749\n",
      "Epoch 214/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.4741\n",
      "Epoch 215/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.4732\n",
      "Epoch 216/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4724\n",
      "Epoch 217/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4716\n",
      "Epoch 218/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4708\n",
      "Epoch 219/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4700\n",
      "Epoch 220/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4692\n",
      "Epoch 221/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4685\n",
      "Epoch 222/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4678\n",
      "Epoch 223/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4682\n",
      "Epoch 224/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.4662\n",
      "Epoch 225/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.4655\n",
      "Epoch 226/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.4659\n",
      "Epoch 227/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4639\n",
      "Epoch 228/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4633\n",
      "Epoch 229/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.4636\n",
      "Epoch 230/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4617\n",
      "Epoch 231/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4610\n",
      "Epoch 232/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4607\n",
      "Epoch 233/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4597\n",
      "Epoch 234/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4593\n",
      "Epoch 235/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4582\n",
      "Epoch 236/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4577\n",
      "Epoch 237/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4568\n",
      "Epoch 238/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4563\n",
      "Epoch 239/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4555\n",
      "Epoch 240/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4551\n",
      "Epoch 241/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4542\n",
      "Epoch 242/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4538\n",
      "Epoch 243/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4529\n",
      "Epoch 244/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4524\n",
      "Epoch 245/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4516\n",
      "Epoch 246/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4512\n",
      "Epoch 247/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4503\n",
      "Epoch 248/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4499\n",
      "Epoch 249/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4491\n",
      "Epoch 250/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4486\n",
      "Tmp result:  0.7383167220376522 {'lr': 0.0001, 'use_weighted_loss': True, 'batch_size': 4, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Epoch 1/1000\n",
      "36/36 [==============================] - 1s 30ms/step - loss: 1.2697\n",
      "Epoch 2/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2294\n",
      "Epoch 3/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.2021\n",
      "Epoch 4/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.1772\n",
      "Epoch 5/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.1504\n",
      "Epoch 6/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.1184\n",
      "Epoch 7/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.0792\n",
      "Epoch 8/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 1.0329\n",
      "Epoch 9/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.9834\n",
      "Epoch 10/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.9346\n",
      "Epoch 11/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.8855\n",
      "Epoch 12/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.8345\n",
      "Epoch 13/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.7820\n",
      "Epoch 14/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7296\n",
      "Epoch 15/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6815\n",
      "Epoch 16/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6410\n",
      "Epoch 17/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6074\n",
      "Epoch 18/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5793\n",
      "Epoch 19/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5558\n",
      "Epoch 20/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5368\n",
      "Epoch 21/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5221\n",
      "Epoch 22/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.5102\n",
      "Epoch 23/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5003\n",
      "Epoch 24/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4922\n",
      "Epoch 25/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4850\n",
      "Epoch 26/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4788\n",
      "Epoch 27/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4736\n",
      "Epoch 28/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4685\n",
      "Epoch 29/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4640\n",
      "Epoch 30/1000\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.4599\n",
      "Epoch 31/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4558\n",
      "Epoch 32/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4520\n",
      "Epoch 33/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4487\n",
      "Epoch 34/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4451\n",
      "Epoch 35/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4416\n",
      "Epoch 36/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4388\n",
      "Epoch 37/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4355\n",
      "Epoch 38/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4321\n",
      "Epoch 39/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4294\n",
      "Epoch 40/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4264\n",
      "Epoch 41/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4231\n",
      "Epoch 42/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4206\n",
      "Epoch 43/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4175\n",
      "Epoch 44/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4146\n",
      "Epoch 45/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4129\n",
      "Epoch 46/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4093\n",
      "Epoch 47/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4075\n",
      "Epoch 48/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4070\n",
      "Epoch 49/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4023\n",
      "Epoch 50/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3998\n",
      "Epoch 51/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3980\n",
      "Epoch 52/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3929\n",
      "Epoch 53/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3897\n",
      "Epoch 54/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3876\n",
      "Epoch 55/1000\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.3841\n",
      "Epoch 56/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3815\n",
      "Epoch 57/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3794\n",
      "Epoch 58/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3762\n",
      "Epoch 59/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3739\n",
      "Epoch 60/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3718\n",
      "Epoch 61/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3682\n",
      "Epoch 62/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3657\n",
      "Epoch 63/1000\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3634\n",
      "Epoch 64/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3586\n",
      "Epoch 65/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3558\n",
      "Epoch 66/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3539\n",
      "Epoch 67/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3505\n",
      "Epoch 68/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3496\n",
      "Epoch 69/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3480\n",
      "Epoch 70/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3426\n",
      "Epoch 71/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3400\n",
      "Epoch 72/1000\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.3417\n",
      "Epoch 73/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3485\n",
      "Epoch 74/1000\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.3444\n",
      "Epoch 75/1000\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.3401\n",
      "Epoch 76/1000\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.3421\n",
      "Epoch 77/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3288\n",
      "Epoch 78/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3225\n",
      "Epoch 79/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3203\n",
      "Epoch 80/1000\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.3193\n",
      "Epoch 81/1000\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.3206\n",
      "Epoch 82/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3227\n",
      "Epoch 83/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3151\n",
      "Epoch 84/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3111\n",
      "Epoch 85/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3094\n",
      "Epoch 86/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3079\n",
      "Epoch 87/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3065\n",
      "Epoch 88/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3067\n",
      "Epoch 89/1000\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3139\n",
      "Epoch 90/1000\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3110\n",
      "Epoch 91/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3043\n",
      "Epoch 92/1000\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.3025\n",
      "Epoch 93/1000\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2944\n",
      "Epoch 94/1000\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2922\n",
      "Epoch 95/1000\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2899\n",
      "Epoch 96/1000\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2877\n",
      "Epoch 97/1000\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2866\n",
      "Epoch 98/1000\n",
      "32/36 [=========================>....] - ETA: 0s - loss: 0.2911"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8b56312983ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    \u001b[0;34m'devices'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                    'reg': reg}\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0mgs_results\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tmp result: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0d3566f353f6>\u001b[0m in \u001b[0;36meval_model_params\u001b[0;34m(params, train_X, train_Y, val_X, val_Y)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdescribed\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0d3566f353f6>\u001b[0m in \u001b[0;36mtrain_model_with_params\u001b[0;34m(params, train_X, train_Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gs_results = []\n",
    "for do in [0.0, 0.2, 0.5]:\n",
    "    for units in [32, 64, 128, 256, 512]:\n",
    "        for reg in [0.0, 0.01, 0.1]:\n",
    "            for n_epochs in [250]:\n",
    "                for lr in [1e-3, 1e-4]:\n",
    "                    K.clear_session()\n",
    "                    tmp_params = {'lr': lr, \n",
    "                                   'use_weighted_loss': USE_WEIGHTED_LOSS,\n",
    "                                   'batch_size': BATCH_SIZE,\n",
    "                                   'dropout': do,\n",
    "                                   'epochs': n_epochs,\n",
    "                                   'n_outputs': len(device_list),\n",
    "                                   'n_features': feature_batch.shape[-1],\n",
    "                                   'lstm_units': units,\n",
    "                                   'devices': device_list,\n",
    "                                   'reg': reg}\n",
    "                    model, val_acc = eval_model_params(tmp_params, feature_batch, label_batch, val_features, val_labels)\n",
    "                    gs_results += [(val_acc, tmp_params)]\n",
    "                    print(\"Tmp result: \", val_acc, tmp_params)\n",
    "                \n",
    "for x in sorted(gs_results, key=lambda x: x[0], reverse=True):\n",
    "    print(\"Acc: {0}, params: {1}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with results from GS to evaluate\n",
    "Given the parameters from the grid search, evaluate also on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tmp result:  0.6961548987411056 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 32, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
    "#best_params = sorted(gs_results, key=lambda x: x[0], reverse=True)[0][1]\n",
    "best_params = {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': BATCH_SIZE, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
    "#best_params = {'lr': 0.0001, \n",
    "#               'use_weighted_loss': True,\n",
    "#               'batch_size': BATCH_SIZE,\n",
    "#               'dropout': 0.0,\n",
    "#               'epochs': 250,\n",
    "#               'n_outputs': len(device_list),\n",
    "#               'n_features': feature_batch.shape[-1],\n",
    "#               'lstm_units': 128,\n",
    "#               'devices': device_list,\n",
    "#               'reg': 0.01}\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 1.2689\n",
      "Epoch 2/250\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 1.0339\n",
      "Epoch 3/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.9006\n",
      "Epoch 4/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.7535\n",
      "Epoch 5/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.6219\n",
      "Epoch 6/250\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.5572\n",
      "Epoch 7/250\n",
      "36/36 [==============================] - 0s 12ms/step - loss: 0.5458\n",
      "Epoch 8/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.5204\n",
      "Epoch 9/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.5120\n",
      "Epoch 10/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4795\n",
      "Epoch 11/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4806\n",
      "Epoch 12/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4709\n",
      "Epoch 13/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.4523\n",
      "Epoch 14/250\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.4516\n",
      "Epoch 15/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.4430\n",
      "Epoch 16/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4429\n",
      "Epoch 17/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4296\n",
      "Epoch 18/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4259\n",
      "Epoch 19/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.4357\n",
      "Epoch 20/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.4229\n",
      "Epoch 21/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.4257\n",
      "Epoch 22/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.4089\n",
      "Epoch 23/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.4131\n",
      "Epoch 24/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4063\n",
      "Epoch 25/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3861\n",
      "Epoch 26/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3794\n",
      "Epoch 27/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3721\n",
      "Epoch 28/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3655\n",
      "Epoch 29/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.3606\n",
      "Epoch 30/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3481\n",
      "Epoch 31/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.3515\n",
      "Epoch 32/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.3561\n",
      "Epoch 33/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3887\n",
      "Epoch 34/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4006\n",
      "Epoch 35/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4203\n",
      "Epoch 36/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.4062\n",
      "Epoch 37/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.3936\n",
      "Epoch 38/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3660\n",
      "Epoch 39/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3438\n",
      "Epoch 40/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.3271\n",
      "Epoch 41/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3168\n",
      "Epoch 42/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.3084\n",
      "Epoch 43/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.3010\n",
      "Epoch 44/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2925\n",
      "Epoch 45/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.2872\n",
      "Epoch 46/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2842\n",
      "Epoch 47/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2766\n",
      "Epoch 48/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2991\n",
      "Epoch 49/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2891\n",
      "Epoch 50/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2883\n",
      "Epoch 51/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2995\n",
      "Epoch 52/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2806\n",
      "Epoch 53/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2791\n",
      "Epoch 54/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2691\n",
      "Epoch 55/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2704\n",
      "Epoch 56/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2634\n",
      "Epoch 57/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2693\n",
      "Epoch 58/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2692\n",
      "Epoch 59/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2750\n",
      "Epoch 60/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2716\n",
      "Epoch 61/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2643\n",
      "Epoch 62/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2538\n",
      "Epoch 63/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2489\n",
      "Epoch 64/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2384\n",
      "Epoch 65/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2394\n",
      "Epoch 66/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2343\n",
      "Epoch 67/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2352\n",
      "Epoch 68/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2387\n",
      "Epoch 69/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2358\n",
      "Epoch 70/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2503\n",
      "Epoch 71/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2469\n",
      "Epoch 72/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2717\n",
      "Epoch 73/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2934\n",
      "Epoch 74/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2551\n",
      "Epoch 75/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2541\n",
      "Epoch 76/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2310\n",
      "Epoch 77/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2247\n",
      "Epoch 78/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2156\n",
      "Epoch 79/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2118\n",
      "Epoch 80/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2060\n",
      "Epoch 81/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2075\n",
      "Epoch 82/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2034\n",
      "Epoch 83/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.2046\n",
      "Epoch 84/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1996\n",
      "Epoch 85/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1990\n",
      "Epoch 86/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.1956\n",
      "Epoch 87/250\n",
      "36/36 [==============================] - 0s 13ms/step - loss: 0.1955\n",
      "Epoch 88/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1923\n",
      "Epoch 89/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1966\n",
      "Epoch 90/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1915\n",
      "Epoch 91/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1952\n",
      "Epoch 92/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1951\n",
      "Epoch 93/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1961\n",
      "Epoch 94/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1981\n",
      "Epoch 95/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1969\n",
      "Epoch 96/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2001\n",
      "Epoch 97/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.2009\n",
      "Epoch 98/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1981\n",
      "Epoch 99/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1902\n",
      "Epoch 100/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1884\n",
      "Epoch 101/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1913\n",
      "Epoch 102/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1814\n",
      "Epoch 103/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1767\n",
      "Epoch 104/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1736\n",
      "Epoch 105/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1701\n",
      "Epoch 106/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1700\n",
      "Epoch 107/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1710\n",
      "Epoch 108/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1701\n",
      "Epoch 109/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1793\n",
      "Epoch 110/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1824\n",
      "Epoch 111/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1916\n",
      "Epoch 112/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1938\n",
      "Epoch 113/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2023\n",
      "Epoch 114/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2350\n",
      "Epoch 115/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2471\n",
      "Epoch 116/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2031\n",
      "Epoch 117/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1894\n",
      "Epoch 118/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1763\n",
      "Epoch 119/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1695\n",
      "Epoch 120/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1621\n",
      "Epoch 121/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1563\n",
      "Epoch 122/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1516\n",
      "Epoch 123/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1507\n",
      "Epoch 124/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1467\n",
      "Epoch 125/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1464\n",
      "Epoch 126/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1449\n",
      "Epoch 127/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1456\n",
      "Epoch 128/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1469\n",
      "Epoch 129/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1430\n",
      "Epoch 130/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1429\n",
      "Epoch 131/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1431\n",
      "Epoch 132/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1563\n",
      "Epoch 133/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1525\n",
      "Epoch 134/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1635\n",
      "Epoch 135/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1537\n",
      "Epoch 136/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1593\n",
      "Epoch 137/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1611\n",
      "Epoch 138/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1695\n",
      "Epoch 139/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2851\n",
      "Epoch 140/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.4715\n",
      "Epoch 141/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.3648\n",
      "Epoch 142/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.3136\n",
      "Epoch 143/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2767\n",
      "Epoch 144/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2663\n",
      "Epoch 145/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2466\n",
      "Epoch 146/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2356\n",
      "Epoch 147/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.2282\n",
      "Epoch 148/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.2222\n",
      "Epoch 149/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2154\n",
      "Epoch 150/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2024\n",
      "Epoch 151/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1973\n",
      "Epoch 152/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1906\n",
      "Epoch 153/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1846\n",
      "Epoch 154/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1810\n",
      "Epoch 155/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1769\n",
      "Epoch 156/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1719\n",
      "Epoch 157/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1686\n",
      "Epoch 158/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1636\n",
      "Epoch 159/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1620\n",
      "Epoch 160/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1614\n",
      "Epoch 161/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1617\n",
      "Epoch 162/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1617\n",
      "Epoch 163/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1570\n",
      "Epoch 164/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1537\n",
      "Epoch 165/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1552\n",
      "Epoch 166/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1516\n",
      "Epoch 167/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1457\n",
      "Epoch 168/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1483\n",
      "Epoch 169/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1526\n",
      "Epoch 170/250\n",
      "36/36 [==============================] - 1s 21ms/step - loss: 0.1698\n",
      "Epoch 171/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.2262\n",
      "Epoch 172/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.2095\n",
      "Epoch 173/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1850\n",
      "Epoch 174/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1735\n",
      "Epoch 175/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1650\n",
      "Epoch 176/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1572\n",
      "Epoch 177/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1481\n",
      "Epoch 178/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1414\n",
      "Epoch 179/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1365\n",
      "Epoch 180/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1322\n",
      "Epoch 181/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1299\n",
      "Epoch 182/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1273\n",
      "Epoch 183/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1263\n",
      "Epoch 184/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1258\n",
      "Epoch 185/250\n",
      "36/36 [==============================] - 1s 14ms/step - loss: 0.1244\n",
      "Epoch 186/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1240\n",
      "Epoch 187/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1256\n",
      "Epoch 188/250\n",
      "36/36 [==============================] - 1s 15ms/step - loss: 0.1270\n",
      "Epoch 189/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1339\n",
      "Epoch 190/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1432\n",
      "Epoch 191/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1417\n",
      "Epoch 192/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1423\n",
      "Epoch 193/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1436\n",
      "Epoch 194/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1344\n",
      "Epoch 195/250\n",
      "36/36 [==============================] - 0s 14ms/step - loss: 0.1340\n",
      "Epoch 196/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1257\n",
      "Epoch 197/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1265\n",
      "Epoch 198/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1252\n",
      "Epoch 199/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1211\n",
      "Epoch 200/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1188\n",
      "Epoch 201/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1159\n",
      "Epoch 202/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1154\n",
      "Epoch 203/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1128\n",
      "Epoch 204/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1152\n",
      "Epoch 205/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1099\n",
      "Epoch 206/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1046\n",
      "Epoch 207/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1050\n",
      "Epoch 208/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1076\n",
      "Epoch 209/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1044\n",
      "Epoch 210/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1029\n",
      "Epoch 211/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1042\n",
      "Epoch 212/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1058\n",
      "Epoch 213/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1041\n",
      "Epoch 214/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1015\n",
      "Epoch 215/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1006\n",
      "Epoch 216/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1059\n",
      "Epoch 217/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1122\n",
      "Epoch 218/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1042\n",
      "Epoch 219/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1033\n",
      "Epoch 220/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1079\n",
      "Epoch 221/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1073\n",
      "Epoch 222/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1124\n",
      "Epoch 223/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1056\n",
      "Epoch 224/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1096\n",
      "Epoch 225/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1312\n",
      "Epoch 226/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1521\n",
      "Epoch 227/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1913\n",
      "Epoch 228/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.1999\n",
      "Epoch 229/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1923\n",
      "Epoch 230/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1799\n",
      "Epoch 231/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1505\n",
      "Epoch 232/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1390\n",
      "Epoch 233/250\n",
      "36/36 [==============================] - 1s 20ms/step - loss: 0.1247\n",
      "Epoch 234/250\n",
      "36/36 [==============================] - 1s 19ms/step - loss: 0.1136\n",
      "Epoch 235/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.1064\n",
      "Epoch 236/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.1005\n",
      "Epoch 237/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.0969\n",
      "Epoch 238/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0928\n",
      "Epoch 239/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0905\n",
      "Epoch 240/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0877\n",
      "Epoch 241/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0864\n",
      "Epoch 242/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.0852\n",
      "Epoch 243/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.0834\n",
      "Epoch 244/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.0834\n",
      "Epoch 245/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0822\n",
      "Epoch 246/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0811\n",
      "Epoch 247/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0807\n",
      "Epoch 248/250\n",
      "36/36 [==============================] - 1s 17ms/step - loss: 0.0807\n",
      "Epoch 249/250\n",
      "36/36 [==============================] - 1s 16ms/step - loss: 0.0834\n",
      "Epoch 250/250\n",
      "36/36 [==============================] - 1s 18ms/step - loss: 0.0805\n",
      "Val acc for model 0.8201827242524908\n"
     ]
    }
   ],
   "source": [
    "model, val_acc = eval_model_params(best_params, feature_batch, label_batch, val_features, val_labels)\n",
    "print(\"Val acc for model\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc:  0.9119856680536295\n"
     ]
    }
   ],
   "source": [
    "test_acc = calc_accuracy(model, best_params, test_features, test_labels)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-07-01 04:00:00', '2016-07-01 05:00:00',\n",
      "               '2016-07-01 06:00:00', '2016-07-01 07:00:00',\n",
      "               '2016-07-01 08:00:00', '2016-07-01 09:00:00',\n",
      "               '2016-07-01 10:00:00', '2016-07-01 11:00:00',\n",
      "               '2016-07-01 12:00:00', '2016-07-01 13:00:00',\n",
      "               ...\n",
      "               '2016-08-12 00:00:00', '2016-08-12 01:00:00',\n",
      "               '2016-08-12 02:00:00', '2016-08-12 03:00:00',\n",
      "               '2016-08-12 04:00:00', '2016-08-12 05:00:00',\n",
      "               '2016-08-12 06:00:00', '2016-08-12 07:00:00',\n",
      "               '2016-08-12 08:00:00', '2016-08-12 09:00:00'],\n",
      "              dtype='datetime64[ns]', length=1014, freq='H')\n",
      "Hours in data:  1014\n",
      "[[ 4.          5.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.          6.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          0.28402367  0.02366864\n",
      "   0.          0.          0.1183432   0.        ]\n",
      " [ 4.          7.          0.          1.          0.          1.\n",
      "   0.          1.          0.          0.1183432   1.06508876  0.07100592\n",
      "   0.09467456  0.14201183  0.52071006  0.        ]\n",
      " [ 4.          8.          0.          1.          1.          1.\n",
      "   1.          1.          0.          0.16568047  1.11242604  0.1183432\n",
      "   0.21301775  0.61538462  0.63905325  0.        ]\n",
      " [ 4.          9.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.1183432   0.87573964  0.14201183\n",
      "   0.35502959  0.49704142  0.56804734  0.        ]\n",
      " [ 4.         10.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.          0.5443787   0.16568047\n",
      "   0.14201183  0.4260355   0.33136095  0.        ]\n",
      " [ 4.         11.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.04733728  0.56804734  0.04733728\n",
      "   0.28402367  0.33136095  0.4260355   0.        ]\n",
      " [ 4.         12.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.09467456  0.94674556  0.14201183\n",
      "   0.1183432   0.09467456  0.47337278  0.        ]\n",
      " [ 4.         13.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.16568047  1.13609467  0.33136095\n",
      "   0.30769231  0.4260355   0.49704142  0.        ]\n",
      " [ 4.         14.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.07100592  0.94674556  0.23668639\n",
      "   0.26035503  0.5443787   0.61538462  0.02366864]]\n",
      "[[0 1 0 0 0 0 0]\n",
      " [0 1 0 1 0 1 0]\n",
      " [0 1 1 1 1 1 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0]]\n",
      "File challenge/data/device_activations_train.csv has 1013 timesteps (hours) until now\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels, _, _ = read_and_preprocess_data(in_file, batch_size=1, device_list=device_list, sequence_start_shift=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc:  0.9579158443398587\n"
     ]
    }
   ],
   "source": [
    "train_acc = calc_accuracy(model, best_params, train_features, train_labels)\n",
    "print(\"Train acc: \", train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models and parameters\n",
    "Save the trained model to be reused in the inference/server script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_occupancies.to_pickle('mean_occupancy.pkl')\n",
    "\n",
    "model.save('model.h5')\n",
    "import json\n",
    "\n",
    "with open('params.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(best_params, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
