{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy Timeseries Forecast\n",
    "## Predict occupancies for future hours from a known history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocess import read_and_preprocess_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import l2\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"challenge/data/device_activations_train.csv\"\n",
    "val_in_file = \"challenge/data/device_activations_val.csv\"\n",
    "test_in_file = \"challenge/data/device_activations_test.csv\"\n",
    "device_list=['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7']\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & preprocess data\n",
    "Load data from csv files, expects different files for training, validation and test (to be specified in the cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-07-01 04:00:00', '2016-07-01 05:00:00',\n",
      "               '2016-07-01 06:00:00', '2016-07-01 07:00:00',\n",
      "               '2016-07-01 08:00:00', '2016-07-01 09:00:00',\n",
      "               '2016-07-01 10:00:00', '2016-07-01 11:00:00',\n",
      "               '2016-07-01 12:00:00', '2016-07-01 13:00:00',\n",
      "               ...\n",
      "               '2016-08-12 00:00:00', '2016-08-12 01:00:00',\n",
      "               '2016-08-12 02:00:00', '2016-08-12 03:00:00',\n",
      "               '2016-08-12 04:00:00', '2016-08-12 05:00:00',\n",
      "               '2016-08-12 06:00:00', '2016-08-12 07:00:00',\n",
      "               '2016-08-12 08:00:00', '2016-08-12 09:00:00'],\n",
      "              dtype='datetime64[ns]', length=1014, freq='H')\n",
      "Hours in data:  1014\n",
      "File challenge/data/device_activations_train.csv has 1013 timesteps (hours) until now\n",
      "initial features shape:  (1013, 16)\n",
      "Full sequence length:  350\n",
      "Sequence 0 has start index 0 and end index 350\n",
      "(350, 16)\n",
      "Sequence 1 has start index 20 and end index 370\n",
      "(350, 16)\n",
      "Sequence 2 has start index 40 and end index 390\n",
      "(350, 16)\n",
      "Sequence 3 has start index 60 and end index 410\n",
      "(350, 16)\n",
      "Sequence 4 has start index 80 and end index 430\n",
      "(350, 16)\n",
      "Sequence 5 has start index 100 and end index 450\n",
      "(350, 16)\n",
      "Sequence 6 has start index 120 and end index 470\n",
      "(350, 16)\n",
      "Sequence 7 has start index 140 and end index 490\n",
      "(350, 16)\n",
      "Sequence 8 has start index 160 and end index 510\n",
      "(350, 16)\n",
      "Sequence 9 has start index 180 and end index 530\n",
      "(350, 16)\n",
      "Sequence 10 has start index 200 and end index 550\n",
      "(350, 16)\n",
      "Sequence 11 has start index 220 and end index 570\n",
      "(350, 16)\n",
      "Sequence 12 has start index 240 and end index 590\n",
      "(350, 16)\n",
      "Sequence 13 has start index 260 and end index 610\n",
      "(350, 16)\n",
      "Sequence 14 has start index 280 and end index 630\n",
      "(350, 16)\n",
      "Sequence 15 has start index 300 and end index 650\n",
      "(350, 16)\n",
      "Sequence 16 has start index 320 and end index 670\n",
      "(350, 16)\n",
      "Sequence 17 has start index 340 and end index 690\n",
      "(350, 16)\n",
      "Sequence 18 has start index 360 and end index 710\n",
      "(350, 16)\n",
      "Sequence 19 has start index 380 and end index 730\n",
      "(350, 16)\n",
      "Sequence 20 has start index 400 and end index 750\n",
      "(350, 16)\n",
      "Sequence 21 has start index 420 and end index 770\n",
      "(350, 16)\n",
      "Sequence 22 has start index 440 and end index 790\n",
      "(350, 16)\n",
      "Sequence 23 has start index 460 and end index 810\n",
      "(350, 16)\n",
      "Sequence 24 has start index 480 and end index 830\n",
      "(350, 16)\n",
      "Sequence 25 has start index 500 and end index 850\n",
      "(350, 16)\n",
      "Sequence 26 has start index 520 and end index 870\n",
      "(350, 16)\n",
      "Sequence 27 has start index 540 and end index 890\n",
      "(350, 16)\n",
      "Sequence 28 has start index 560 and end index 910\n",
      "(350, 16)\n",
      "Sequence 29 has start index 580 and end index 930\n",
      "(350, 16)\n",
      "Sequence 30 has start index 600 and end index 950\n",
      "(350, 16)\n",
      "Sequence 31 has start index 620 and end index 970\n",
      "(350, 16)\n",
      "Features sequences shape:  (32, 7, 50, 16)\n",
      "Labels sequences shape:  (32, 7, 50, 7)\n",
      "Feature batch shape:  (224, 50, 16)\n",
      "Label batch shape:  (224, 50, 7)\n"
     ]
    }
   ],
   "source": [
    "feature_batch, label_batch, device_list, mean_occupancies = read_and_preprocess_data(in_file, batch_size=BATCH_SIZE, device_list=device_list, sequence_start_shift=20)\n",
    "print(\"Feature batch shape: \", feature_batch.shape)\n",
    "print(\"Label batch shape: \", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean occupancy visualization per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.04395604 0.1487743  0.31445478 0.36517329 0.31107354 0.40236686\n",
      " 0.28064243 0.43956044 0.44632291 0.51394759 0.44294167 0.16906171\n",
      " 0.03381234 0.03043111 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.04057481 0.21301775 0.30431107 0.36855452 0.40912933 0.41251057\n",
      " 0.26373626 0.40912933 0.45984784 0.51394759 0.4057481  0.16568047\n",
      " 0.04057481 0.0202874  0.04057481 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.07100592 0.34826712 0.44632291 0.44294167 0.34150465 0.3956044\n",
      " 0.38884193 0.48689772 0.51394759 0.54775993 0.44632291 0.29416737\n",
      " 0.06762468 0.00676247 0.         0.00338123 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.05748098 0.30769231 0.49027895 0.5545224  0.52747253 0.44970414\n",
      " 0.45646661 0.52409129 0.55114117 0.534235   0.37531699 0.22316145\n",
      " 0.08453085 0.02704987 0.00676247 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00338123 0.\n",
      " 0.06086221 0.2874049  0.40912933 0.36517329 0.22992392 0.24344886\n",
      " 0.2671175  0.40912933 0.38546069 0.31783601 0.18258664 0.06424345\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.00338123 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.00338123 0.0202874  0.00676247 0.\n",
      " 0.         0.         0.         0.         0.         0.00338123\n",
      " 0.01690617 0.         0.         0.         0.00338123 0.\n",
      " 0.         0.         0.00338123 0.         0.         0.        ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_occupancy_per_hour = mean_occupancies.to_numpy().reshape([168, 7]).sum(axis=1) / 7\n",
    "labels = range(len(mean_occupancy_per_hour))\n",
    "print(mean_occupancy_per_hour)\n",
    "plt.bar(labels, mean_occupancy_per_hour, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-08-12 10:00:00', '2016-08-12 11:00:00',\n",
      "               '2016-08-12 12:00:00', '2016-08-12 13:00:00',\n",
      "               '2016-08-12 14:00:00', '2016-08-12 15:00:00',\n",
      "               '2016-08-12 16:00:00', '2016-08-12 17:00:00',\n",
      "               '2016-08-12 18:00:00', '2016-08-12 19:00:00',\n",
      "               ...\n",
      "               '2016-08-24 00:00:00', '2016-08-24 01:00:00',\n",
      "               '2016-08-24 02:00:00', '2016-08-24 03:00:00',\n",
      "               '2016-08-24 04:00:00', '2016-08-24 05:00:00',\n",
      "               '2016-08-24 06:00:00', '2016-08-24 07:00:00',\n",
      "               '2016-08-24 08:00:00', '2016-08-24 09:00:00'],\n",
      "              dtype='datetime64[ns]', length=288, freq='H')\n",
      "Hours in data:  288\n",
      "File challenge/data/device_activations_val.csv has 287 timesteps (hours) until now\n",
      "PREALLOCATE END TIME:  None\n",
      "PREALLOCATE hour_interval_start_end:  DatetimeIndex(['2016-08-24 10:00:00', '2016-08-24 11:00:00',\n",
      "               '2016-08-24 12:00:00', '2016-08-24 13:00:00',\n",
      "               '2016-08-24 14:00:00', '2016-08-24 15:00:00',\n",
      "               '2016-08-24 16:00:00', '2016-08-24 17:00:00',\n",
      "               '2016-08-24 18:00:00', '2016-08-24 19:00:00',\n",
      "               ...\n",
      "               '2016-08-31 08:00:00', '2016-08-31 09:00:00',\n",
      "               '2016-08-31 10:00:00', '2016-08-31 11:00:00',\n",
      "               '2016-08-31 12:00:00', '2016-08-31 13:00:00',\n",
      "               '2016-08-31 14:00:00', '2016-08-31 15:00:00',\n",
      "               '2016-08-31 16:00:00', '2016-08-31 17:00:00'],\n",
      "              dtype='datetime64[ns]', length=176, freq='H')\n",
      "Hours in data:  176\n",
      "File challenge/data/device_activations_test.csv has 175 timesteps (hours) until now\n"
     ]
    }
   ],
   "source": [
    "val_features, val_labels, _, _ = read_and_preprocess_data(val_in_file, batch_size=1, device_list=device_list)\n",
    "test_features, test_labels, _, _ = read_and_preprocess_data(test_in_file, batch_size=1, device_list=device_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ratio of positive to negative labels per device to adjust loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio of positive to negative outputs per device:  [0.0825     0.25125    0.16464286 0.24125    0.235625   0.34696429\n",
      " 0.03776786]\n"
     ]
    }
   ],
   "source": [
    "def calc_ratio_positive_outputs_per_device(labels):\n",
    "    ratio_per_device = np.sum(labels, axis=0) / labels.shape[0]\n",
    "    print(\"Ratio of positive to negative outputs per device: \", ratio_per_device)\n",
    "    return np.array(ratio_per_device)\n",
    "ratio_positive_outputs_per_device = calc_ratio_positive_outputs_per_device(label_batch.reshape([-1, label_batch.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted loss: Scales up loss for positive labels per-device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own weighted loss to combat label imbalance\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    out = -(y_true * K.log(y_pred + 1e-5) / ratio_positive_outputs_per_device + (1.0 - y_true) * K.log(1.0 - y_pred + 1e-5))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \"\"\"\n",
    "    Creates an LSTM-based model given the parameters: 'lr', 'do', 'reg', 'lstm_units', 'n_outputs', 'n_features', 'use_weighted_loss'.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['lstm_units'], batch_input_shape=(params['batch_size'], None, params['n_features']), return_sequences=True, stateful=True, kernel_regularizer=l2(params['reg'])))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(params['n_outputs'], activation='sigmoid'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=params['lr'])\n",
    "    model.compile(loss=weighted_loss if params['use_weighted_loss'] else 'binary_crossentropy', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_24h(model, features, labels):\n",
    "    \"\"\"\n",
    "    Predicts a model's output for given features and the following 24 hours, reusing predictions as inputs\n",
    "    \"\"\"\n",
    "    predictions = np.squeeze(model.predict(np.expand_dims(features, 0), batch_size=1))  # (n_timesteps, n_outputs)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    last_features = np.squeeze(features)[-1]\n",
    "    last_predictions = tmp_prediction = predictions[-1]\n",
    "\n",
    "    tmp_features = np.array(last_features)\n",
    "    tmp_mean_occupancies = [mean_occupancies.loc[(tmp_features[0] * 24 + tmp_features[1], 'device_' + str(i + 1)), \n",
    "                                                 'mean_occupancy'] for i in range(len(device_list))]\n",
    "\n",
    "    for i in range(24):\n",
    "        tmp_mean_occupancies = [mean_occupancies.loc[(tmp_features[0] * 24 + tmp_features[1], 'device_' + str(j + 1)), \n",
    "                                                     'mean_occupancy'] for j in range(len(device_list))]\n",
    "        tmp_features = np.concatenate([tmp_features[:2], np.squeeze(tmp_prediction), tmp_mean_occupancies])\n",
    "        \n",
    "        # Increment time features\n",
    "        if tmp_features[1] == 23:\n",
    "            tmp_features[0] = (tmp_features[0] + 1) % 7\n",
    "        tmp_features[1] = (tmp_features[1] + 1) % 24\n",
    "        \n",
    "        tmp_prediction = np.round(model.predict(np.reshape(tmp_features, [1, 1, len(tmp_features)])))\n",
    "        \n",
    "        all_predictions += [tmp_prediction]\n",
    "\n",
    "    return np.concatenate(all_predictions)\n",
    "\n",
    "def calc_accuracy(model, params, test_X, test_Y):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of a model on a test (or validation) set. Generates sub-sequences of the test set to test on.\n",
    "    The model gets 48+i hours as inputs before having to predict the next 24 hours (i in range(48, n - 24) with \n",
    "    n = hours in test set). This gives more \"effective\" test set size than splitting the test set in non-overlapping\n",
    "    sequences. Depending on the real world scenario, the assumption of at least 48 hours of input might be relaxed/\n",
    "    strictened.\n",
    "    \"\"\"\n",
    "    # Hack around Keras batch size restriction (to have same for training/test)\n",
    "    model.save('tmp_model.h5')\n",
    "    test_params = dict(params)\n",
    "    test_params['batch_size'] = 1\n",
    "    test_model = create_model(test_params)\n",
    "    test_model.load_weights('tmp_model.h5')\n",
    "    os.remove('tmp_model.h5')\n",
    "    n = test_X.shape[1]\n",
    "    acc_accumulated = 0.0\n",
    "    for i in range(48, n - 24):\n",
    "        predictions = np.squeeze(predict_24h(test_model, test_X[0, :i], test_Y[0, :i]))\n",
    "        true_labels = test_Y[0, i:i+24]\n",
    "        \n",
    "        acc = np.sum(np.round(predictions) == true_labels) / predictions.size\n",
    "        acc_accumulated += acc\n",
    "    \n",
    "    return acc_accumulated / (n-26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model_with_params(params, train_X, train_Y):\n",
    "    model = create_model(params)\n",
    "    history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0, shuffle=False)\n",
    "    return model\n",
    "\n",
    "def eval_model_params(params, train_X, train_Y, val_X, val_Y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model on a given validation set by calculating its accuracy on a 24h prediction basis as \n",
    "    described in calc_accuracy().\n",
    "    \"\"\"\n",
    "    model = train_model_with_params(params, feature_batch, label_batch)\n",
    "    val_acc = calc_accuracy(model, params, val_features, val_labels)\n",
    "    return model, val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter (Grid) Search\n",
    "Usually I use the BayesianOptimization framework to efficiently calculate and evaluate hyperparameter spaces as GS\n",
    "gets out of hand quickly in terms of computational efficiency. In this case I simply ran out of time for that,\n",
    "so GS had to suffice. RS would have been an option, but I'm not too much of a fan and the training times in this \n",
    "project were \"ok\", so GS was my choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_results = []\n",
    "for do in [0.0, 0.2, 0.5]:\n",
    "    for units in [32, 64, 128, 256, 512]:\n",
    "        for reg in [0.0, 0.01, 0.1]:\n",
    "            for n_epochs in [50, 250, 1000]:\n",
    "                for lr in [1e-3, 1e-4]:\n",
    "                    K.clear_session()\n",
    "                    tmp_params = {'lr': lr, \n",
    "                                   'use_weighted_loss': USE_WEIGHTED_LOSS,\n",
    "                                   'batch_size': BATCH_SIZE,\n",
    "                                   'dropout': do,\n",
    "                                   'epochs': n_epochs,\n",
    "                                   'n_outputs': len(device_list),\n",
    "                                   'n_features': feature_batch.shape[-1],\n",
    "                                   'lstm_units': units,\n",
    "                                   'devices': device_list,\n",
    "                                   'reg': reg}\n",
    "                    model, val_acc = eval_model_params(tmp_params, feature_batch, label_batch, val_features, val_labels)\n",
    "                    gs_results += [(val_acc, tmp_params)]\n",
    "                    print(\"Tmp result: \", val_acc, tmp_params)\n",
    "                \n",
    "for x in sorted(gs_results, key=lambda x: x[0], reverse=True):\n",
    "    print(\"Acc: {0}, params: {1}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with results from GS to evaluate\n",
    "Given the parameters from the grid search, evaluate also on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tmp result:  0.6961548987411056 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 32, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
    "#best_params = sorted(gs_results, key=lambda x: x[0], reverse=True)[0][1]\n",
    "best_params = {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 32, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
    "#best_params = {'lr': 0.0001, \n",
    "#               'use_weighted_loss': True,\n",
    "#               'batch_size': BATCH_SIZE,\n",
    "#               'dropout': 0.0,\n",
    "#               'epochs': 250,\n",
    "#               'n_outputs': len(device_list),\n",
    "#               'n_features': feature_batch.shape[-1],\n",
    "#               'lstm_units': 128,\n",
    "#               'devices': device_list,\n",
    "#               'reg': 0.01}\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model, val_acc = eval_model_params(best_params, feature_batch, label_batch, val_features, val_labels)\n",
    "print(\"Val acc for model\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = calc_accuracy(model, best_params, test_features, test_labels)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models and parameters\n",
    "Save the trained model to be reused in the inference/server script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_occupancies.to_pickle('mean_occupancy.pkl')\n",
    "\n",
    "model.save('model.h5')\n",
    "import json\n",
    "\n",
    "with open('params.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(best_params, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
