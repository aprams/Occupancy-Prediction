{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocess import read_and_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"challenge/data/device_activations_small.csv\"\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File challenge/data/device_activations_small.csv has 125 timesteps (hours)\n",
      "initial features shape:  (125, 8)\n",
      "Full sequence length:  60\n",
      "Sequence 0 has start index 0 and end index 60\n",
      "(60, 8)\n",
      "Sequence 1 has start index 4 and end index 64\n",
      "(60, 8)\n",
      "Sequence 2 has start index 8 and end index 68\n",
      "(60, 8)\n",
      "Sequence 3 has start index 12 and end index 72\n",
      "(60, 8)\n",
      "Sequence 4 has start index 16 and end index 76\n",
      "(60, 8)\n",
      "Sequence 5 has start index 20 and end index 80\n",
      "(60, 8)\n",
      "Sequence 6 has start index 24 and end index 84\n",
      "(60, 8)\n",
      "Sequence 7 has start index 28 and end index 88\n",
      "(60, 8)\n",
      "Sequence 8 has start index 32 and end index 92\n",
      "(60, 8)\n",
      "Sequence 9 has start index 36 and end index 96\n",
      "(60, 8)\n",
      "Sequence 10 has start index 40 and end index 100\n",
      "(60, 8)\n",
      "Sequence 11 has start index 44 and end index 104\n",
      "(60, 8)\n",
      "Sequence 12 has start index 48 and end index 108\n",
      "(60, 8)\n",
      "Sequence 13 has start index 52 and end index 112\n",
      "(60, 8)\n",
      "Sequence 14 has start index 56 and end index 116\n",
      "(60, 8)\n",
      "Sequence 15 has start index 60 and end index 120\n",
      "(60, 8)\n",
      "Features sequences shape:  (16, 3, 20, 8)\n",
      "Labels sequences shape:  (16, 3, 20, 6)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "[[ 4.  4.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4.  5.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4.  6.  1.  0.  0.  0.  0.  0.]\n",
      " [ 4.  7.  1.  0.  1.  0.  1.  0.]\n",
      " [ 4.  8.  1.  1.  1.  1.  1.  0.]\n",
      " [ 4.  9.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 10.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 11.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 12.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 13.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 14.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 15.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 16.  1.  0.  0.  0.  0.  0.]\n",
      " [ 4. 17.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 18.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 19.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 20.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 21.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 22.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 23.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 4.  8.  1.  1.  1.  1.  1.  0.]\n",
      " [ 4.  9.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 10.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 11.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 12.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 13.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 14.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 15.  1.  1.  0.  0.  0.  0.]\n",
      " [ 4. 16.  1.  0.  0.  0.  0.  0.]\n",
      " [ 4. 17.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 18.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 19.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 20.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 21.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 22.  0.  0.  0.  0.  0.  0.]\n",
      " [ 4. 23.  0.  0.  0.  0.  0.  0.]\n",
      " [ 5.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 5.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 5.  2.  0.  0.  0.  0.  0.  0.]\n",
      " [ 5.  3.  0.  0.  0.  0.  0.  0.]]\n",
      "[[ 1.  8.  0.  0.  1.  1.  1.  1.]\n",
      " [ 1.  9.  0.  0.  1.  1.  1.  0.]\n",
      " [ 1. 10.  0.  0.  1.  1.  1.  0.]\n",
      " [ 1. 11.  0.  0.  1.  1.  1.  0.]\n",
      " [ 1. 12.  0.  0.  1.  1.  1.  0.]\n",
      " [ 1. 13.  0.  0.  1.  1.  1.  0.]\n",
      " [ 1. 14.  0.  1.  1.  1.  1.  0.]\n",
      " [ 1. 15.  1.  1.  1.  1.  1.  0.]\n",
      " [ 1. 16.  1.  1.  1.  1.  1.  0.]\n",
      " [ 1. 17.  1.  1.  1.  1.  1.  1.]\n",
      " [ 1. 18.  0.  1.  0.  0.  1.  0.]\n",
      " [ 1. 19.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1. 20.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1. 21.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. 22.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1. 23.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  2.  0.  0.  0.  0.  0.  0.]\n",
      " [ 2.  3.  0.  0.  0.  0.  0.  0.]]\n",
      "Feature batch:  (48, 20, 8)\n",
      "Label batch:  (48, 20, 6)\n"
     ]
    }
   ],
   "source": [
    "feature_batch, label_batch, device_list = read_and_preprocess_data(in_file, batch_size=BATCH_SIZE)\n",
    "print(feature_batch[0])\n",
    "print(feature_batch[1])\n",
    "print(feature_batch[-1])\n",
    "print(\"Feature batch: \", feature_batch.shape)\n",
    "print(\"Label batch: \", label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len:  32\n",
      "(32, 20, 8)\n",
      "(16, 20, 8)\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "train_len = int(train_ratio * len(feature_batch) // BATCH_SIZE) * BATCH_SIZE\n",
    "print(\"Train len: \", train_len)\n",
    "\n",
    "train_feature_batch = feature_batch[:train_len]\n",
    "test_feature_batch = feature_batch[train_len:]\n",
    "train_label_batch = label_batch[:train_len]\n",
    "test_label_batch = label_batch[train_len:]\n",
    "\n",
    "print(train_feature_batch.shape)\n",
    "print(test_feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive outputs per device:  [0.16979167 0.1875     0.11770833 0.10104167 0.13645833 0.046875  ]\n"
     ]
    }
   ],
   "source": [
    "def calc_ratio_positive_outputs_per_device(labels):\n",
    "    ratio_per_device = np.sum(labels, axis=0) / labels.shape[0]\n",
    "    print(\"Percentage of positive outputs per device: \", ratio_per_device)\n",
    "    return np.array(ratio_per_device)\n",
    "ratio_positive_outputs_per_device = calc_ratio_positive_outputs_per_device(label_batch.reshape([-1, label_batch.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own weighted loss to combat label imbalance\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    out = -(y_true * K.log(y_pred + 1e-5) / ratio_positive_outputs_per_device + (1.0 - y_true) * K.log(1.0 - y_pred + 1e-5))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    n_outputs = len(device_list)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, batch_input_shape=(params['batch_size'], None, feature_batch.shape[-1]), return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(n_outputs, activation='sigmoid'))\n",
    "    model.compile(loss=weighted_loss if params['use_weighted_loss'] else 'binary_crossentropy', optimizer=params['optimizer'])\n",
    "    return model\n",
    "\n",
    "training_params = {'optimizer': 'adam', \n",
    "                   'use_weighted_loss': USE_WEIGHTED_LOSS,\n",
    "                   'batch_size': BATCH_SIZE,\n",
    "                   'dropout': 0.0,\n",
    "                   'epochs': 300}\n",
    "\n",
    "#model = create_model(training_params)\n",
    "#model.fit(train_feature_batch, train_label_batch, epochs=training_params['epochs'], batch_size=training_params['batch_size'], verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_batch_flattened = test_feature_batch.reshape([-1, *test_feature_batch.shape[-1:]])\n",
    "test_label_batch_flattened = test_label_batch.reshape([-1, *test_label_batch.shape[-1:]])\n",
    "test_feature_batch_expanded = test_feature_batch_flattened if len(test_feature_batch_flattened.shape) == 3 else np.expand_dims(test_feature_batch_flattened, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 16 samples\n",
      "Epoch 1/300\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1.0084 - val_loss: 2.0172\n",
      "Epoch 2/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9287 - val_loss: 2.0010\n",
      "Epoch 3/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8714 - val_loss: 1.9358\n",
      "Epoch 4/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8195 - val_loss: 1.8256\n",
      "Epoch 5/300\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7699 - val_loss: 1.6918\n",
      "Epoch 6/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.7253 - val_loss: 1.5620\n",
      "Epoch 7/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6880 - val_loss: 1.4519\n",
      "Epoch 8/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6581 - val_loss: 1.3650\n",
      "Epoch 9/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6315 - val_loss: 1.2988\n",
      "Epoch 10/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.6066 - val_loss: 1.2473\n",
      "Epoch 11/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5830 - val_loss: 1.1981\n",
      "Epoch 12/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5581 - val_loss: 1.1402\n",
      "Epoch 13/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5321 - val_loss: 1.0813\n",
      "Epoch 14/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5068 - val_loss: 1.0345\n",
      "Epoch 15/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4844 - val_loss: 1.0017\n",
      "Epoch 16/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4678 - val_loss: 0.9769\n",
      "Epoch 17/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4467 - val_loss: 0.9622\n",
      "Epoch 18/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4229 - val_loss: 0.9716\n",
      "Epoch 19/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4096 - val_loss: 1.0006\n",
      "Epoch 20/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4002 - val_loss: 1.0000\n",
      "Epoch 21/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3802 - val_loss: 0.9619\n",
      "Epoch 22/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3634 - val_loss: 0.9277\n",
      "Epoch 23/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3540 - val_loss: 0.9001\n",
      "Epoch 24/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3471 - val_loss: 0.8707\n",
      "Epoch 25/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3357 - val_loss: 0.8448\n",
      "Epoch 26/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3276 - val_loss: 0.8394\n",
      "Epoch 27/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3235 - val_loss: 0.8473\n",
      "Epoch 28/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3183 - val_loss: 0.8463\n",
      "Epoch 29/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3093 - val_loss: 0.8596\n",
      "Epoch 30/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3010 - val_loss: 0.8974\n",
      "Epoch 31/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2950 - val_loss: 0.9242\n",
      "Epoch 32/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2864 - val_loss: 0.9122\n",
      "Epoch 33/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2814 - val_loss: 0.9173\n",
      "Epoch 34/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2812 - val_loss: 0.9419\n",
      "Epoch 35/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2773 - val_loss: 0.9422\n",
      "Epoch 36/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2700 - val_loss: 0.9560\n",
      "Epoch 37/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2690 - val_loss: 0.9731\n",
      "Epoch 38/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2608 - val_loss: 0.9417\n",
      "Epoch 39/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2567 - val_loss: 0.9683\n",
      "Epoch 40/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2584 - val_loss: 1.0248\n",
      "Epoch 41/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2458 - val_loss: 0.9707\n",
      "Epoch 42/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2422 - val_loss: 0.9930\n",
      "Epoch 43/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2554 - val_loss: 1.0322\n",
      "Epoch 44/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2342 - val_loss: 0.9847\n",
      "Epoch 45/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2403 - val_loss: 0.9921\n",
      "Epoch 46/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2425 - val_loss: 1.0685\n",
      "Epoch 47/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2443 - val_loss: 1.0290\n",
      "Epoch 48/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2248 - val_loss: 0.9825\n",
      "Epoch 49/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2229 - val_loss: 0.9724\n",
      "Epoch 50/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2331 - val_loss: 0.9988\n",
      "Epoch 51/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2174 - val_loss: 1.0034\n",
      "Epoch 52/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2282 - val_loss: 1.0300\n",
      "Epoch 53/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2248 - val_loss: 1.1180\n",
      "Epoch 54/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2306 - val_loss: 1.1161\n",
      "Epoch 55/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2052 - val_loss: 0.9865\n",
      "Epoch 56/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2109 - val_loss: 0.9262\n",
      "Epoch 57/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2024 - val_loss: 0.9011\n",
      "Epoch 58/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2235 - val_loss: 0.9186\n",
      "Epoch 59/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2008 - val_loss: 0.8882\n",
      "Epoch 60/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2069 - val_loss: 0.9403\n",
      "Epoch 61/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1971 - val_loss: 0.9788\n",
      "Epoch 62/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2019 - val_loss: 1.0369\n",
      "Epoch 63/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2040 - val_loss: 1.0236\n",
      "Epoch 64/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1913 - val_loss: 1.0213\n",
      "Epoch 65/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1910 - val_loss: 1.0543\n",
      "Epoch 66/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1967 - val_loss: 1.1398\n",
      "Epoch 67/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1965 - val_loss: 1.0888\n",
      "Epoch 68/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1791 - val_loss: 0.9704\n",
      "Epoch 69/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1795 - val_loss: 0.8874\n",
      "Epoch 70/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1844 - val_loss: 0.8799\n",
      "Epoch 71/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1790 - val_loss: 0.9670\n",
      "Epoch 72/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1782 - val_loss: 1.0250\n",
      "Epoch 73/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1797 - val_loss: 1.0579\n",
      "Epoch 74/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1728 - val_loss: 1.0797\n",
      "Epoch 75/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1697 - val_loss: 1.0731\n",
      "Epoch 76/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1722 - val_loss: 1.0476\n",
      "Epoch 77/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1683 - val_loss: 1.0760\n",
      "Epoch 78/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1677 - val_loss: 1.1407\n",
      "Epoch 79/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1654 - val_loss: 1.1423\n",
      "Epoch 80/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1605 - val_loss: 1.0847\n",
      "Epoch 81/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1574 - val_loss: 1.0670\n",
      "Epoch 82/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 909us/step - loss: 0.1627 - val_loss: 1.0998\n",
      "Epoch 83/300\n",
      "32/32 [==============================] - 0s 944us/step - loss: 0.1534 - val_loss: 1.1241\n",
      "Epoch 84/300\n",
      "32/32 [==============================] - 0s 867us/step - loss: 0.1511 - val_loss: 1.1837\n",
      "Epoch 85/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1525 - val_loss: 1.1823\n",
      "Epoch 86/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1491 - val_loss: 1.0624\n",
      "Epoch 87/300\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.1492 - val_loss: 0.9940\n",
      "Epoch 88/300\n",
      "32/32 [==============================] - 0s 977us/step - loss: 0.1530 - val_loss: 1.1114\n",
      "Epoch 89/300\n",
      "32/32 [==============================] - 0s 982us/step - loss: 0.1451 - val_loss: 1.1587\n",
      "Epoch 90/300\n",
      "32/32 [==============================] - 0s 996us/step - loss: 0.1423 - val_loss: 1.1193\n",
      "Epoch 91/300\n",
      "32/32 [==============================] - 0s 989us/step - loss: 0.1381 - val_loss: 1.1493\n",
      "Epoch 92/300\n",
      "32/32 [==============================] - 0s 959us/step - loss: 0.1398 - val_loss: 1.2268\n",
      "Epoch 93/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 1.2315\n",
      "Epoch 94/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1327 - val_loss: 1.2242\n",
      "Epoch 95/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1331 - val_loss: 1.2377\n",
      "Epoch 96/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1312 - val_loss: 1.2446\n",
      "Epoch 97/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1290 - val_loss: 1.2366\n",
      "Epoch 98/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1330 - val_loss: 1.2671\n",
      "Epoch 99/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1287 - val_loss: 1.2669\n",
      "Epoch 100/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1268 - val_loss: 1.2522\n",
      "Epoch 101/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1250 - val_loss: 1.2333\n",
      "Epoch 102/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1287 - val_loss: 1.2843\n",
      "Epoch 103/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1239 - val_loss: 1.3512\n",
      "Epoch 104/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1266 - val_loss: 1.2334\n",
      "Epoch 105/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1241 - val_loss: 1.2870\n",
      "Epoch 106/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1478 - val_loss: 1.3652\n",
      "Epoch 107/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1508 - val_loss: 1.1538\n",
      "Epoch 108/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1322 - val_loss: 1.3255\n",
      "Epoch 109/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1623 - val_loss: 1.3952\n",
      "Epoch 110/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1420 - val_loss: 1.2698\n",
      "Epoch 111/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1898 - val_loss: 0.9843\n",
      "Epoch 112/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1753 - val_loss: 1.0094\n",
      "Epoch 113/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1683 - val_loss: 1.0673\n",
      "Epoch 114/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1657 - val_loss: 1.2138\n",
      "Epoch 115/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1633 - val_loss: 1.3067\n",
      "Epoch 116/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1381 - val_loss: 1.1682\n",
      "Epoch 117/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1650 - val_loss: 1.0279\n",
      "Epoch 118/300\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.1550 - val_loss: 0.9794\n",
      "Epoch 119/300\n",
      "32/32 [==============================] - 0s 980us/step - loss: 0.1421 - val_loss: 0.9872\n",
      "Epoch 120/300\n",
      "32/32 [==============================] - 0s 928us/step - loss: 0.1564 - val_loss: 1.0009\n",
      "Epoch 121/300\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.1448 - val_loss: 0.8937\n",
      "Epoch 122/300\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.1318 - val_loss: 0.8699\n",
      "Epoch 123/300\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.1360 - val_loss: 0.9218\n",
      "Epoch 124/300\n",
      "32/32 [==============================] - 0s 959us/step - loss: 0.1354 - val_loss: 1.0922\n",
      "Epoch 125/300\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.1318 - val_loss: 1.1823\n",
      "Epoch 126/300\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.1334 - val_loss: 1.2679\n",
      "Epoch 127/300\n",
      "32/32 [==============================] - 0s 983us/step - loss: 0.1323 - val_loss: 1.2957\n",
      "Epoch 128/300\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.1203 - val_loss: 1.2352\n",
      "Epoch 129/300\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.1224 - val_loss: 1.2333\n",
      "Epoch 130/300\n",
      "32/32 [==============================] - 0s 932us/step - loss: 0.1308 - val_loss: 1.2120\n",
      "Epoch 131/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1162 - val_loss: 1.1723\n",
      "Epoch 132/300\n",
      "32/32 [==============================] - 0s 931us/step - loss: 0.1138 - val_loss: 1.1553\n",
      "Epoch 133/300\n",
      "32/32 [==============================] - 0s 988us/step - loss: 0.1146 - val_loss: 1.1204\n",
      "Epoch 134/300\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.1218 - val_loss: 1.0590\n",
      "Epoch 135/300\n",
      "32/32 [==============================] - 0s 963us/step - loss: 0.1135 - val_loss: 1.0064\n",
      "Epoch 136/300\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.1086 - val_loss: 1.0582\n",
      "Epoch 137/300\n",
      "32/32 [==============================] - 0s 978us/step - loss: 0.1122 - val_loss: 1.2324\n",
      "Epoch 138/300\n",
      "32/32 [==============================] - 0s 976us/step - loss: 0.1077 - val_loss: 1.2304\n",
      "Epoch 139/300\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.1080 - val_loss: 1.2608\n",
      "Epoch 140/300\n",
      "32/32 [==============================] - 0s 944us/step - loss: 0.1186 - val_loss: 1.3100\n",
      "Epoch 141/300\n",
      "32/32 [==============================] - 0s 941us/step - loss: 0.1188 - val_loss: 1.3150\n",
      "Epoch 142/300\n",
      "32/32 [==============================] - 0s 908us/step - loss: 0.1201 - val_loss: 1.3016\n",
      "Epoch 143/300\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.1181 - val_loss: 1.2934\n",
      "Epoch 144/300\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.1171 - val_loss: 1.2879\n",
      "Epoch 145/300\n",
      "32/32 [==============================] - 0s 887us/step - loss: 0.1098 - val_loss: 1.2766\n",
      "Epoch 146/300\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.1031 - val_loss: 1.2721\n",
      "Epoch 147/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1070 - val_loss: 1.2810\n",
      "Epoch 148/300\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.1083 - val_loss: 1.3082\n",
      "Epoch 149/300\n",
      "32/32 [==============================] - 0s 930us/step - loss: 0.1026 - val_loss: 1.3025\n",
      "Epoch 150/300\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.1005 - val_loss: 1.2825\n",
      "Epoch 151/300\n",
      "32/32 [==============================] - 0s 949us/step - loss: 0.1040 - val_loss: 1.2468\n",
      "Epoch 152/300\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.1014 - val_loss: 1.2085\n",
      "Epoch 153/300\n",
      "32/32 [==============================] - 0s 914us/step - loss: 0.0964 - val_loss: 1.2313\n",
      "Epoch 154/300\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.1009 - val_loss: 1.3268\n",
      "Epoch 155/300\n",
      "32/32 [==============================] - 0s 992us/step - loss: 0.0983 - val_loss: 1.4104\n",
      "Epoch 156/300\n",
      "32/32 [==============================] - 0s 918us/step - loss: 0.0949 - val_loss: 1.4415\n",
      "Epoch 157/300\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.0938 - val_loss: 1.4825\n",
      "Epoch 158/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0997 - val_loss: 1.4845\n",
      "Epoch 159/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0924 - val_loss: 1.4675\n",
      "Epoch 160/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0901 - val_loss: 1.4629\n",
      "Epoch 161/300\n",
      "32/32 [==============================] - 0s 940us/step - loss: 0.0961 - val_loss: 1.4623\n",
      "Epoch 162/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 976us/step - loss: 0.0884 - val_loss: 1.4399\n",
      "Epoch 163/300\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.0873 - val_loss: 1.4353\n",
      "Epoch 164/300\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.0951 - val_loss: 1.4821\n",
      "Epoch 165/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0874 - val_loss: 1.5136\n",
      "Epoch 166/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0863 - val_loss: 1.5503\n",
      "Epoch 167/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 1.5745\n",
      "Epoch 168/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0913 - val_loss: 1.5627\n",
      "Epoch 169/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 1.5603\n",
      "Epoch 170/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0857 - val_loss: 1.5938\n",
      "Epoch 171/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0880 - val_loss: 1.5920\n",
      "Epoch 172/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0862 - val_loss: 1.5891\n",
      "Epoch 173/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0842 - val_loss: 1.6300\n",
      "Epoch 174/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 1.6725\n",
      "Epoch 175/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0834 - val_loss: 1.6522\n",
      "Epoch 176/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0854 - val_loss: 1.6561\n",
      "Epoch 177/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0915 - val_loss: 1.6642\n",
      "Epoch 178/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0836 - val_loss: 1.6050\n",
      "Epoch 179/300\n",
      "32/32 [==============================] - 0s 988us/step - loss: 0.0821 - val_loss: 1.6292\n",
      "Epoch 180/300\n",
      "32/32 [==============================] - 0s 964us/step - loss: 0.0906 - val_loss: 1.6990\n",
      "Epoch 181/300\n",
      "32/32 [==============================] - 0s 958us/step - loss: 0.0842 - val_loss: 1.6691\n",
      "Epoch 182/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0809 - val_loss: 1.6928\n",
      "Epoch 183/300\n",
      "32/32 [==============================] - 0s 998us/step - loss: 0.0914 - val_loss: 1.7021\n",
      "Epoch 184/300\n",
      "32/32 [==============================] - 0s 997us/step - loss: 0.1003 - val_loss: 1.4564\n",
      "Epoch 185/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0972 - val_loss: 1.6984\n",
      "Epoch 186/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0988 - val_loss: 1.6807\n",
      "Epoch 187/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0947 - val_loss: 1.6012\n",
      "Epoch 188/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1030 - val_loss: 1.4945\n",
      "Epoch 189/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1067 - val_loss: 1.6698\n",
      "Epoch 190/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1128 - val_loss: 1.7898\n",
      "Epoch 191/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0968 - val_loss: 1.6935\n",
      "Epoch 192/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1018 - val_loss: 1.5561\n",
      "Epoch 193/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0980 - val_loss: 1.4853\n",
      "Epoch 194/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1080 - val_loss: 1.4889\n",
      "Epoch 195/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0883 - val_loss: 1.3827\n",
      "Epoch 196/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0882 - val_loss: 1.3563\n",
      "Epoch 197/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0871 - val_loss: 1.5698\n",
      "Epoch 198/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0846 - val_loss: 1.6613\n",
      "Epoch 199/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0810 - val_loss: 1.6774\n",
      "Epoch 200/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0794 - val_loss: 1.6822\n",
      "Epoch 201/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0745 - val_loss: 1.6785\n",
      "Epoch 202/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0780 - val_loss: 1.6518\n",
      "Epoch 203/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0766 - val_loss: 1.6157\n",
      "Epoch 204/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0761 - val_loss: 1.5759\n",
      "Epoch 205/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0757 - val_loss: 1.5407\n",
      "Epoch 206/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0786 - val_loss: 1.5166\n",
      "Epoch 207/300\n",
      "32/32 [==============================] - 0s 993us/step - loss: 0.0781 - val_loss: 1.5050\n",
      "Epoch 208/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0725 - val_loss: 1.5337\n",
      "Epoch 209/300\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.0698 - val_loss: 1.5962\n",
      "Epoch 210/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0699 - val_loss: 1.6802\n",
      "Epoch 211/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0702 - val_loss: 1.7028\n",
      "Epoch 212/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0696 - val_loss: 1.7112\n",
      "Epoch 213/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0684 - val_loss: 1.7455\n",
      "Epoch 214/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0692 - val_loss: 1.7503\n",
      "Epoch 215/300\n",
      "32/32 [==============================] - 0s 960us/step - loss: 0.0690 - val_loss: 1.6969\n",
      "Epoch 216/300\n",
      "32/32 [==============================] - 0s 970us/step - loss: 0.0705 - val_loss: 1.6634\n",
      "Epoch 217/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0692 - val_loss: 1.6577\n",
      "Epoch 218/300\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0679 - val_loss: 1.7122\n",
      "Epoch 219/300\n",
      "32/32 [==============================] - 0s 960us/step - loss: 0.0670 - val_loss: 1.7615\n",
      "Epoch 220/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0668 - val_loss: 1.7546\n",
      "Epoch 221/300\n",
      "32/32 [==============================] - 0s 994us/step - loss: 0.0673 - val_loss: 1.7706\n",
      "Epoch 222/300\n",
      "32/32 [==============================] - 0s 961us/step - loss: 0.0667 - val_loss: 1.7922\n",
      "Epoch 223/300\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.0659 - val_loss: 1.7762\n",
      "Epoch 224/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0660 - val_loss: 1.7541\n",
      "Epoch 225/300\n",
      "32/32 [==============================] - 0s 916us/step - loss: 0.0680 - val_loss: 1.7510\n",
      "Epoch 226/300\n",
      "32/32 [==============================] - 0s 996us/step - loss: 0.0682 - val_loss: 1.7898\n",
      "Epoch 227/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0664 - val_loss: 1.8251\n",
      "Epoch 228/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0660 - val_loss: 1.8107\n",
      "Epoch 229/300\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.0667 - val_loss: 1.8030\n",
      "Epoch 230/300\n",
      "32/32 [==============================] - 0s 953us/step - loss: 0.0666 - val_loss: 1.8447\n",
      "Epoch 231/300\n",
      "32/32 [==============================] - 0s 969us/step - loss: 0.0650 - val_loss: 1.8940\n",
      "Epoch 232/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0648 - val_loss: 1.8990\n",
      "Epoch 233/300\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.0659 - val_loss: 1.8799\n",
      "Epoch 234/300\n",
      "32/32 [==============================] - 0s 983us/step - loss: 0.0668 - val_loss: 1.8954\n",
      "Epoch 235/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0656 - val_loss: 1.9483\n",
      "Epoch 236/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0650 - val_loss: 1.9899\n",
      "Epoch 237/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0658 - val_loss: 1.9892\n",
      "Epoch 238/300\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.0658 - val_loss: 1.9986\n",
      "Epoch 239/300\n",
      "32/32 [==============================] - 0s 973us/step - loss: 0.0646 - val_loss: 2.0205\n",
      "Epoch 240/300\n",
      "32/32 [==============================] - 0s 966us/step - loss: 0.0655 - val_loss: 2.0229\n",
      "Epoch 241/300\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.0664 - val_loss: 2.0281\n",
      "Epoch 242/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 993us/step - loss: 0.0649 - val_loss: 2.0396\n",
      "Epoch 243/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0638 - val_loss: 2.0461\n",
      "Epoch 244/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0655 - val_loss: 2.0623\n",
      "Epoch 245/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0641 - val_loss: 2.0808\n",
      "Epoch 246/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 2.0790\n",
      "Epoch 247/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0644 - val_loss: 2.1179\n",
      "Epoch 248/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0643 - val_loss: 2.1199\n",
      "Epoch 249/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0629 - val_loss: 2.0894\n",
      "Epoch 250/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0647 - val_loss: 2.0617\n",
      "Epoch 251/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0673 - val_loss: 2.1020\n",
      "Epoch 252/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0640 - val_loss: 2.1999\n",
      "Epoch 253/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0621 - val_loss: 2.1803\n",
      "Epoch 254/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0620 - val_loss: 2.1106\n",
      "Epoch 255/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0626 - val_loss: 2.1162\n",
      "Epoch 256/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0611 - val_loss: 2.1485\n",
      "Epoch 257/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0607 - val_loss: 2.0676\n",
      "Epoch 258/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0628 - val_loss: 2.0254\n",
      "Epoch 259/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0608 - val_loss: 2.0555\n",
      "Epoch 260/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0593 - val_loss: 2.1326\n",
      "Epoch 261/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0612 - val_loss: 2.1584\n",
      "Epoch 262/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0591 - val_loss: 2.1425\n",
      "Epoch 263/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0594 - val_loss: 2.1205\n",
      "Epoch 264/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0645 - val_loss: 2.3246\n",
      "Epoch 265/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1982 - val_loss: 1.9699\n",
      "Epoch 266/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2410 - val_loss: 1.4608\n",
      "Epoch 267/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4049 - val_loss: 0.8336\n",
      "Epoch 268/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3338 - val_loss: 0.7775\n",
      "Epoch 269/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3011 - val_loss: 0.9756\n",
      "Epoch 270/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2056 - val_loss: 1.2726\n",
      "Epoch 271/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1713 - val_loss: 1.4737\n",
      "Epoch 272/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1691 - val_loss: 1.5989\n",
      "Epoch 273/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1671 - val_loss: 1.6520\n",
      "Epoch 274/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1506 - val_loss: 1.7071\n",
      "Epoch 275/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1412 - val_loss: 1.7645\n",
      "Epoch 276/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1340 - val_loss: 1.7620\n",
      "Epoch 277/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1249 - val_loss: 1.6625\n",
      "Epoch 278/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1196 - val_loss: 1.6231\n",
      "Epoch 279/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1151 - val_loss: 1.5928\n",
      "Epoch 280/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1117 - val_loss: 1.6013\n",
      "Epoch 281/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1056 - val_loss: 1.6144\n",
      "Epoch 282/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1033 - val_loss: 1.6243\n",
      "Epoch 283/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1092 - val_loss: 1.6549\n",
      "Epoch 284/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1041 - val_loss: 1.7218\n",
      "Epoch 285/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0943 - val_loss: 1.7566\n",
      "Epoch 286/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0907 - val_loss: 1.7439\n",
      "Epoch 287/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0895 - val_loss: 1.7062\n",
      "Epoch 288/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0896 - val_loss: 1.7162\n",
      "Epoch 289/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0872 - val_loss: 1.7894\n",
      "Epoch 290/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0853 - val_loss: 1.8614\n",
      "Epoch 291/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0811 - val_loss: 1.8439\n",
      "Epoch 292/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0806 - val_loss: 1.8437\n",
      "Epoch 293/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0797 - val_loss: 1.8924\n",
      "Epoch 294/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0779 - val_loss: 1.9374\n",
      "Epoch 295/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0772 - val_loss: 1.8973\n",
      "Epoch 296/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0803 - val_loss: 1.8870\n",
      "Epoch 297/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0817 - val_loss: 1.9009\n",
      "Epoch 298/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0805 - val_loss: 1.9195\n",
      "Epoch 299/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0788 - val_loss: 1.9332\n",
      "Epoch 300/300\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.0777 - val_loss: 1.9403\n",
      "1.940321445465088\n"
     ]
    }
   ],
   "source": [
    "def eval_model_params(params, train_X, train_Y, test_X, test_Y):\n",
    "    model = create_model(training_params)\n",
    "    history = model.fit(train_X, train_Y, validation_data=(test_X, test_Y), epochs=params['epochs'], batch_size=params['batch_size'], verbose=1, shuffle=False)\n",
    "    return model, history.history['val_loss'][-1]\n",
    "\n",
    "\n",
    "model, result = eval_model_params(training_params, train_feature_batch, train_label_batch, test_feature_batch, test_label_batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(320, 1, 8)\n",
      "(320, 6)\n",
      "(320, 6)\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1.]]\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "print(test_feature_batch_expanded.shape)\n",
    "\n",
    "predictions = model.predict(test_feature_batch_expanded, batch_size=BATCH_SIZE)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "print(predictions.shape)\n",
    "print(test_label_batch_flattened.shape)\n",
    "\n",
    "print(np.round(predictions)[:-24])\n",
    "print(test_label_batch_flattened[:-24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy:  0.7239583333333334\n",
      "Val accuracy per device::  [0.65625  0.659375 0.7125   0.734375 0.69375  0.8875  ]\n",
      "% of 1 prediction outputs 0.19270833333333334\n",
      "% of 1 label outputs 0.22291666666666668\n"
     ]
    }
   ],
   "source": [
    "print(\"Val accuracy: \", np.sum(np.round(predictions) == test_label_batch_flattened) / predictions.size) \n",
    "print(\"Val accuracy per device:: \", np.sum(np.round(predictions) == test_label_batch_flattened, axis=0) / predictions.shape[0]) \n",
    "\n",
    "print(\"% of 1 prediction outputs\", np.sum(np.round(predictions)) / predictions.size) \n",
    "print(\"% of 1 label outputs\", np.sum(np.round(test_label_batch_flattened)) / test_label_batch_flattened.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "keras.losses.weighted_loss = weighted_loss\n",
    "\n",
    "test_model_params = dict(training_params)\n",
    "test_model_params['batch_size'] = 1\n",
    "\n",
    "#test_model = load_model(\"model.h5)\n",
    "test_model = create_model(test_model_params)\n",
    "test_model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, in_file):\n",
    "    test_features, test_labels, device_list = read_and_preprocess_data(in_file, batch_size=1)\n",
    "    print(feature_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    predictions = np.concatenate(model.predict(feature_batch, batch_size=1), axis=0)\n",
    "    print(np.round(predictions))\n",
    "    print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File challenge/data/device_activations_small.csv has 125 timesteps (hours)\n",
      "(48, 20, 8)\n",
      "(48, 20, 6)\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "[[[0. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 0. 0. 0. 0.]\n",
      "  [1. 0. 1. 0. 1. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0. 0. 0. 0.]\n",
      "  [1. 1. 0. 0. 0. 0.]\n",
      "  [1. 1. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 1. 0. 0. 0. 0.]\n",
      "  [1. 1. 0. 0. 0. 0.]\n",
      "  [1. 1. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  ...\n",
      "  [0. 1. 0. 0. 1. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 1. 0. 1. 1.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  [0. 0. 1. 1. 1. 0.]\n",
      "  ...\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "test(test_model, in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_24h(model, in_file):\n",
    "    feature_batch, label_batch, device_list = read_and_preprocess_data(in_file, batch_size=1)\n",
    "    print(feature_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    predictions = np.concatenate(model.predict(feature_batch, batch_size=1), axis=0)\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    last_features = feature_batch[-1, -1]\n",
    "    last_predictions = tmp_prediction = predictions[-1]\n",
    "    \n",
    "    tmp_features = np.array(last_features)\n",
    "    tmp_features = np.concatenate([tmp_features[:2], last_predictions])\n",
    "    for i in range(24):\n",
    "        print(tmp_features)\n",
    "        #print(tmp_prediction)\n",
    "        tmp_prediction = model.predict(np.reshape(tmp_features, [1, 1, len(tmp_features)]))\n",
    "        tmp_features = np.concatenate([tmp_features[:2], tmp_prediction[0, 0]])\n",
    "        \n",
    "        # Increment time features\n",
    "        if tmp_features[1] == 23:\n",
    "            tmp_features[0] = (tmp_features[0] + 1) % 7\n",
    "        tmp_features[1] = (tmp_features[1] + 1) % 24\n",
    "        all_predictions += [tmp_prediction]\n",
    "        \n",
    "    return np.round(np.concatenate(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File challenge/data/device_activations_smaller.csv has 101 timesteps (hours)\n",
      "(1, 101, 8)\n",
      "(1, 101, 6)\n",
      "[1.         8.         0.4905757  0.52424389 0.96454465 0.92275691\n",
      " 0.98310781 0.99123621]\n",
      "[1.         9.         0.55714828 0.78615808 0.9591285  0.90150356\n",
      " 0.98801219 0.98402739]\n",
      "[ 1.         10.          0.63118595  0.93917292  0.93931311  0.77016407\n",
      "  0.99010706  0.92764586]\n",
      "[ 1.         11.          0.55081654  0.98028439  0.78443027  0.29767621\n",
      "  0.98191595  0.46374705]\n",
      "[ 1.         12.          0.21132828  0.97757304  0.17721161  0.03371891\n",
      "  0.82908475  0.04795123]\n",
      "[1.00000000e+00 1.30000000e+01 8.39546844e-02 8.90964866e-01\n",
      " 1.30461482e-02 5.69049967e-03 1.42055795e-01 5.86819742e-03]\n",
      "[1.00000000e+00 1.40000000e+01 4.88298014e-02 4.13005680e-01\n",
      " 2.13941513e-03 2.51513347e-03 1.12842107e-02 2.30253744e-03]\n",
      "[1.00000000e+00 1.50000000e+01 3.02042477e-02 1.21193908e-01\n",
      " 9.58568940e-04 2.21730210e-03 4.23723739e-03 2.37838388e-03]\n",
      "[1.00000000e+00 1.60000000e+01 1.62274484e-02 5.07574938e-02\n",
      " 7.40132411e-04 2.43604300e-03 3.88755137e-03 3.39956768e-03]\n",
      "[1.00000000e+00 1.70000000e+01 8.47319607e-03 2.83527020e-02\n",
      " 6.46174653e-04 2.40071886e-03 4.09596507e-03 4.75029880e-03]\n",
      "[1.00000000e+00 1.80000000e+01 5.42616844e-03 2.00056620e-02\n",
      " 5.70630829e-04 2.14181188e-03 4.11700457e-03 6.32337341e-03]\n",
      "[1.00000000e+00 1.90000000e+01 4.56009479e-03 1.78197473e-02\n",
      " 5.31791418e-04 1.88517850e-03 4.43104655e-03 8.57644342e-03]\n",
      "[1.00000000e+00 2.00000000e+01 4.22083354e-03 1.69492494e-02\n",
      " 5.05269621e-04 1.60395016e-03 5.09441132e-03 1.14181396e-02]\n",
      "[1.00000000e+00 2.10000000e+01 3.99588048e-03 1.64441187e-02\n",
      " 4.92458523e-04 1.34455238e-03 5.79863274e-03 1.40468227e-02]\n",
      "[1.00000000e+00 2.20000000e+01 3.93666374e-03 1.67217627e-02\n",
      " 5.05641627e-04 1.15126197e-03 6.26417669e-03 1.53603423e-02]\n",
      "[1.00000000e+00 2.30000000e+01 4.04217979e-03 1.75384805e-02\n",
      " 5.39720815e-04 1.01732847e-03 6.22272538e-03 1.50448829e-02]\n",
      "[2.00000000e+00 0.00000000e+00 4.26419126e-03 1.87560990e-02\n",
      " 5.78761625e-04 9.30343987e-04 5.58139384e-03 1.28948959e-02]\n",
      "[2.00000000e+00 1.00000000e+00 4.00594948e-03 8.65085330e-03\n",
      " 1.04252459e-03 1.09279109e-03 1.31605500e-02 8.13544821e-03]\n",
      "[2.00000000e+00 2.00000000e+00 3.29711847e-03 4.54869121e-03\n",
      " 1.93616468e-03 1.52832596e-03 3.17494385e-02 1.02584790e-02]\n",
      "[2.00000000e+00 3.00000000e+00 4.08670306e-03 3.85932135e-03\n",
      " 3.80679360e-03 2.91077723e-03 5.49727827e-02 1.63082369e-02]\n",
      "[2.         4.         0.00545328 0.00517576 0.00662998 0.00644668\n",
      " 0.07704398 0.02910805]\n",
      "[2.         5.         0.00691044 0.00568809 0.01207997 0.00909126\n",
      " 0.07368812 0.04644226]\n",
      "[2.         6.         0.0105894  0.00813316 0.02197547 0.01355552\n",
      " 0.06921069 0.05228031]\n",
      "[2.         7.         0.0159772  0.01089562 0.02606247 0.01912116\n",
      " 0.0493569  0.04885467]\n"
     ]
    }
   ],
   "source": [
    "test_file = \"challenge/data/device_activations_smaller.csv\"\n",
    "future_predictions = predict_next_24h(test_model, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File challenge/data/device_activations_small.csv has 125 timesteps (hours)\n",
      "(125, 6)\n",
      "(24, 1, 6)\n",
      "[[1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 0 1 0]\n",
      " [0 1 0 0 1 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "[[0 0 1 1 1 0]\n",
      " [0 0 1 1 1 0]\n",
      " [0 0 1 1 1 0]\n",
      " [0 0 1 1 1 0]\n",
      " [0 1 1 1 1 0]\n",
      " [1 1 1 1 1 0]\n",
      " [1 1 1 1 1 0]\n",
      " [1 1 1 1 1 1]\n",
      " [0 1 0 0 1 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0]\n",
      " [1 1 1 0 1 0]\n",
      " [1 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "label_file = \"challenge/data/device_activations_small.csv\"\n",
    "test_model.reset_states()\n",
    "feature_batch, label_batch, device_list = read_and_preprocess_data(label_file, batch_size=1)\n",
    "label_batch = label_batch.squeeze()\n",
    "print(label_batch.shape)\n",
    "print(future_predictions.shape)\n",
    "future_predictions = np.squeeze(future_predictions.astype(np.int64))\n",
    "print(future_predictions)\n",
    "print(label_batch[-24:])\n",
    "future_labels = label_batch[-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 6)\n",
      "(24, 6)\n",
      "Test accuracy:  0.6319444444444444\n",
      "Test accuracy per device::  [0.58333333 0.45833333 0.66666667 0.66666667 0.625      0.79166667]\n"
     ]
    }
   ],
   "source": [
    "print(future_predictions.shape)\n",
    "print(future_labels.shape)\n",
    "\n",
    "print(\"Test accuracy: \", np.sum(np.round(future_predictions) == future_labels) / future_labels.size) \n",
    "print(\"Test accuracy per device:: \", np.sum(np.round(future_predictions) == future_labels, axis=0) / future_labels.shape[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[  0   1]\n",
      "   [  2   3]\n",
      "   [  4   5]\n",
      "   [  6   7]\n",
      "   [  8   9]\n",
      "   [ 10  11]\n",
      "   [ 12  13]\n",
      "   [ 14  15]\n",
      "   [ 16  17]\n",
      "   [ 18  19]\n",
      "   [ 20  21]\n",
      "   [ 22  23]\n",
      "   [ 24  25]\n",
      "   [ 26  27]\n",
      "   [ 28  29]\n",
      "   [ 30  31]]\n",
      "\n",
      "  [[ 32  33]\n",
      "   [ 34  35]\n",
      "   [ 36  37]\n",
      "   [ 38  39]\n",
      "   [ 40  41]\n",
      "   [ 42  43]\n",
      "   [ 44  45]\n",
      "   [ 46  47]\n",
      "   [ 48  49]\n",
      "   [ 50  51]\n",
      "   [ 52  53]\n",
      "   [ 54  55]\n",
      "   [ 56  57]\n",
      "   [ 58  59]\n",
      "   [ 60  61]\n",
      "   [ 62  63]]]\n",
      "\n",
      "\n",
      " [[[ 64  65]\n",
      "   [ 66  67]\n",
      "   [ 68  69]\n",
      "   [ 70  71]\n",
      "   [ 72  73]\n",
      "   [ 74  75]\n",
      "   [ 76  77]\n",
      "   [ 78  79]\n",
      "   [ 80  81]\n",
      "   [ 82  83]\n",
      "   [ 84  85]\n",
      "   [ 86  87]\n",
      "   [ 88  89]\n",
      "   [ 90  91]\n",
      "   [ 92  93]\n",
      "   [ 94  95]]\n",
      "\n",
      "  [[ 96  97]\n",
      "   [ 98  99]\n",
      "   [100 101]\n",
      "   [102 103]\n",
      "   [104 105]\n",
      "   [106 107]\n",
      "   [108 109]\n",
      "   [110 111]\n",
      "   [112 113]\n",
      "   [114 115]\n",
      "   [116 117]\n",
      "   [118 119]\n",
      "   [120 121]\n",
      "   [122 123]\n",
      "   [124 125]\n",
      "   [126 127]]]\n",
      "\n",
      "\n",
      " [[[128 129]\n",
      "   [130 131]\n",
      "   [132 133]\n",
      "   [134 135]\n",
      "   [136 137]\n",
      "   [138 139]\n",
      "   [140 141]\n",
      "   [142 143]\n",
      "   [144 145]\n",
      "   [146 147]\n",
      "   [148 149]\n",
      "   [150 151]\n",
      "   [152 153]\n",
      "   [154 155]\n",
      "   [156 157]\n",
      "   [158 159]]\n",
      "\n",
      "  [[160 161]\n",
      "   [162 163]\n",
      "   [164 165]\n",
      "   [166 167]\n",
      "   [168 169]\n",
      "   [170 171]\n",
      "   [172 173]\n",
      "   [174 175]\n",
      "   [176 177]\n",
      "   [178 179]\n",
      "   [180 181]\n",
      "   [182 183]\n",
      "   [184 185]\n",
      "   [186 187]\n",
      "   [188 189]\n",
      "   [190 191]]]\n",
      "\n",
      "\n",
      " [[[192 193]\n",
      "   [194 195]\n",
      "   [196 197]\n",
      "   [198 199]\n",
      "   [200 201]\n",
      "   [202 203]\n",
      "   [204 205]\n",
      "   [206 207]\n",
      "   [208 209]\n",
      "   [210 211]\n",
      "   [212 213]\n",
      "   [214 215]\n",
      "   [216 217]\n",
      "   [218 219]\n",
      "   [220 221]\n",
      "   [222 223]]\n",
      "\n",
      "  [[224 225]\n",
      "   [226 227]\n",
      "   [228 229]\n",
      "   [230 231]\n",
      "   [232 233]\n",
      "   [234 235]\n",
      "   [236 237]\n",
      "   [238 239]\n",
      "   [240 241]\n",
      "   [242 243]\n",
      "   [244 245]\n",
      "   [246 247]\n",
      "   [248 249]\n",
      "   [250 251]\n",
      "   [252 253]\n",
      "   [254 255]]]]\n",
      "(4, 2, 16, 2)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "[[[  0.   1.]\n",
      "  [  2.   3.]\n",
      "  [  4.   5.]\n",
      "  [  6.   7.]\n",
      "  [  8.   9.]\n",
      "  [ 10.  11.]\n",
      "  [ 12.  13.]\n",
      "  [ 14.  15.]\n",
      "  [ 16.  17.]\n",
      "  [ 18.  19.]\n",
      "  [ 20.  21.]\n",
      "  [ 22.  23.]\n",
      "  [ 24.  25.]\n",
      "  [ 26.  27.]\n",
      "  [ 28.  29.]\n",
      "  [ 30.  31.]]\n",
      "\n",
      " [[ 64.  65.]\n",
      "  [ 66.  67.]\n",
      "  [ 68.  69.]\n",
      "  [ 70.  71.]\n",
      "  [ 72.  73.]\n",
      "  [ 74.  75.]\n",
      "  [ 76.  77.]\n",
      "  [ 78.  79.]\n",
      "  [ 80.  81.]\n",
      "  [ 82.  83.]\n",
      "  [ 84.  85.]\n",
      "  [ 86.  87.]\n",
      "  [ 88.  89.]\n",
      "  [ 90.  91.]\n",
      "  [ 92.  93.]\n",
      "  [ 94.  95.]]\n",
      "\n",
      " [[128. 129.]\n",
      "  [130. 131.]\n",
      "  [132. 133.]\n",
      "  [134. 135.]\n",
      "  [136. 137.]\n",
      "  [138. 139.]\n",
      "  [140. 141.]\n",
      "  [142. 143.]\n",
      "  [144. 145.]\n",
      "  [146. 147.]\n",
      "  [148. 149.]\n",
      "  [150. 151.]\n",
      "  [152. 153.]\n",
      "  [154. 155.]\n",
      "  [156. 157.]\n",
      "  [158. 159.]]\n",
      "\n",
      " [[192. 193.]\n",
      "  [194. 195.]\n",
      "  [196. 197.]\n",
      "  [198. 199.]\n",
      "  [200. 201.]\n",
      "  [202. 203.]\n",
      "  [204. 205.]\n",
      "  [206. 207.]\n",
      "  [208. 209.]\n",
      "  [210. 211.]\n",
      "  [212. 213.]\n",
      "  [214. 215.]\n",
      "  [216. 217.]\n",
      "  [218. 219.]\n",
      "  [220. 221.]\n",
      "  [222. 223.]]\n",
      "\n",
      " [[ 32.  33.]\n",
      "  [ 34.  35.]\n",
      "  [ 36.  37.]\n",
      "  [ 38.  39.]\n",
      "  [ 40.  41.]\n",
      "  [ 42.  43.]\n",
      "  [ 44.  45.]\n",
      "  [ 46.  47.]\n",
      "  [ 48.  49.]\n",
      "  [ 50.  51.]\n",
      "  [ 52.  53.]\n",
      "  [ 54.  55.]\n",
      "  [ 56.  57.]\n",
      "  [ 58.  59.]\n",
      "  [ 60.  61.]\n",
      "  [ 62.  63.]]\n",
      "\n",
      " [[ 96.  97.]\n",
      "  [ 98.  99.]\n",
      "  [100. 101.]\n",
      "  [102. 103.]\n",
      "  [104. 105.]\n",
      "  [106. 107.]\n",
      "  [108. 109.]\n",
      "  [110. 111.]\n",
      "  [112. 113.]\n",
      "  [114. 115.]\n",
      "  [116. 117.]\n",
      "  [118. 119.]\n",
      "  [120. 121.]\n",
      "  [122. 123.]\n",
      "  [124. 125.]\n",
      "  [126. 127.]]\n",
      "\n",
      " [[160. 161.]\n",
      "  [162. 163.]\n",
      "  [164. 165.]\n",
      "  [166. 167.]\n",
      "  [168. 169.]\n",
      "  [170. 171.]\n",
      "  [172. 173.]\n",
      "  [174. 175.]\n",
      "  [176. 177.]\n",
      "  [178. 179.]\n",
      "  [180. 181.]\n",
      "  [182. 183.]\n",
      "  [184. 185.]\n",
      "  [186. 187.]\n",
      "  [188. 189.]\n",
      "  [190. 191.]]\n",
      "\n",
      " [[224. 225.]\n",
      "  [226. 227.]\n",
      "  [228. 229.]\n",
      "  [230. 231.]\n",
      "  [232. 233.]\n",
      "  [234. 235.]\n",
      "  [236. 237.]\n",
      "  [238. 239.]\n",
      "  [240. 241.]\n",
      "  [242. 243.]\n",
      "  [244. 245.]\n",
      "  [246. 247.]\n",
      "  [248. 249.]\n",
      "  [250. 251.]\n",
      "  [252. 253.]\n",
      "  [254. 255.]]]\n"
     ]
    }
   ],
   "source": [
    "# Sandbox\n",
    "n = 128\n",
    "n_sequences = 4\n",
    "sequence_length = 16\n",
    "a = np.arange(256)\n",
    "n_minibatches = n // sequence_length // n_sequences\n",
    "b = sequences = np.reshape(a, [n_sequences, n_minibatches, sequence_length, 2])\n",
    "print(b)\n",
    "print(b.shape)\n",
    "sequence_shift = n // sequence_length\n",
    "\n",
    "mini_batch_features_arr_shape = [n_minibatches * n_sequences, sequence_length, 2]\n",
    "mini_batch_features = np.zeros(mini_batch_features_arr_shape)\n",
    "mini_batch_labels_arr_shape = [n_minibatches * n_sequences, sequence_length, 2]\n",
    "mini_batch_labels = np.zeros(mini_batch_labels_arr_shape)\n",
    "for i in range(n_minibatches):\n",
    "    for j in range(n_sequences):\n",
    "        mini_batch_features[i * n_sequences + j] = b[j, i]\n",
    "        mini_batch_labels[i * n_sequences + j] = b[j, i]\n",
    "        print(i * n_sequences + j)\n",
    "        \n",
    "print(mini_batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mini_batch_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ea9251417bca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mindexes\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msequence_shift\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mini_batch_count' is not defined"
     ]
    }
   ],
   "source": [
    "indexes=[]\n",
    "for x in range(mini_batch_count):\n",
    "    for i in range(batch_size):\n",
    "        for j in range(sequence_length):\n",
    "            indexes += [i * sequence_shift + x * sequence_length + j]\n",
    "print(np.reshape(indexes, [mini_batch_count, batch_size, sequence_length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
