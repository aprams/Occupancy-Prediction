{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocess import read_and_preprocess_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"challenge/data/device_activations_small.csv\"\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaaaaaaa 2016-07-06 09:58:22\n",
      "File challenge/data/device_activations_small.csv has 125 timesteps (hours) until now\n",
      "initial features shape:  (125, 8)\n",
      "Full sequence length:  -400\n",
      "Sequence 0 has start index 0 and end index -350\n",
      "(0, 8)\n",
      "Sequence 1 has start index 30 and end index -320\n",
      "(0, 8)\n",
      "Sequence 2 has start index 60 and end index -290\n",
      "(0, 8)\n",
      "Sequence 3 has start index 90 and end index -260\n",
      "(0, 8)\n",
      "Sequence 4 has start index 120 and end index -230\n",
      "(0, 8)\n",
      "Sequence 5 has start index 150 and end index -200\n",
      "(0, 8)\n",
      "Sequence 6 has start index 180 and end index -170\n",
      "(0, 8)\n",
      "Sequence 7 has start index 210 and end index -140\n",
      "(0, 8)\n",
      "Sequence 8 has start index 240 and end index -110\n",
      "(0, 8)\n",
      "Sequence 9 has start index 270 and end index -80\n",
      "(0, 8)\n",
      "Sequence 10 has start index 300 and end index -50\n",
      "(0, 8)\n",
      "Sequence 11 has start index 330 and end index -20\n",
      "(0, 8)\n",
      "Sequence 12 has start index 360 and end index 10\n",
      "(0, 8)\n",
      "Sequence 13 has start index 390 and end index 40\n",
      "(0, 8)\n",
      "Sequence 14 has start index 420 and end index 70\n",
      "(0, 8)\n",
      "Sequence 15 has start index 450 and end index 100\n",
      "(0, 8)\n",
      "Features sequences shape:  (16, 0, 100, 8)\n",
      "Labels sequences shape:  (16, 0, 100, 6)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "negative dimensions are not allowed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f84d5f549e3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature batch: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/preprocess.py\u001b[0m in \u001b[0;36mread_and_preprocess_data\u001b[0;34m(in_file, current_time, batch_size)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_timeseries_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/preprocess.py\u001b[0m in \u001b[0;36mcreate_timeseries_batches\u001b[0;34m(features, labels, sequence_length, sequence_start_shift, n_sequences)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mmini_batch_features_arr_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_minibatches\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mmini_batch_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_features_arr_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mmini_batch_labels_arr_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mn_minibatches\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0mmini_batch_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_labels_arr_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: negative dimensions are not allowed"
     ]
    }
   ],
   "source": [
    "feature_batch, label_batch, device_list = read_and_preprocess_data(in_file, batch_size=BATCH_SIZE)\n",
    "print(feature_batch[0])\n",
    "print(feature_batch[1])\n",
    "print(feature_batch[-1])\n",
    "print(\"Feature batch: \", feature_batch.shape)\n",
    "print(\"Label batch: \", label_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.9\n",
    "train_len = int(train_ratio * len(feature_batch) // BATCH_SIZE) * BATCH_SIZE\n",
    "print(\"Train len: \", train_len)\n",
    "\n",
    "train_feature_batch = feature_batch[:train_len]\n",
    "test_feature_batch = feature_batch[train_len:]\n",
    "train_label_batch = label_batch[:train_len]\n",
    "test_label_batch = label_batch[train_len:]\n",
    "\n",
    "print(train_feature_batch.shape)\n",
    "print(test_feature_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ratio_positive_outputs_per_device(labels):\n",
    "    ratio_per_device = np.sum(labels, axis=0) / labels.shape[0]\n",
    "    print(\"Percentage of positive outputs per device: \", ratio_per_device)\n",
    "    return np.array(ratio_per_device)\n",
    "ratio_positive_outputs_per_device = calc_ratio_positive_outputs_per_device(label_batch.reshape([-1, label_batch.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own weighted loss to combat label imbalance\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    out = -(y_true * K.log(y_pred + 1e-5) / ratio_positive_outputs_per_device + (1.0 - y_true) * K.log(1.0 - y_pred + 1e-5))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['lstm_units'], batch_input_shape=(params['batch_size'], None, params['n_features']), return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(params['n_outputs'], activation='sigmoid'))\n",
    "    model.compile(loss=weighted_loss if params['use_weighted_loss'] else 'binary_crossentropy', optimizer=params['optimizer'])\n",
    "    return model\n",
    "\n",
    "training_params = {'optimizer': 'adam', \n",
    "                   'use_weighted_loss': USE_WEIGHTED_LOSS,\n",
    "                   'batch_size': BATCH_SIZE,\n",
    "                   'dropout': 0.5,\n",
    "                   'epochs': 50,\n",
    "                   'n_outputs': len(device_list),\n",
    "                   'n_features': feature_batch.shape[-1],\n",
    "                   'lstm_units': 256,\n",
    "                   'devices': device_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_batch_flattened = test_feature_batch.reshape([-1, *test_feature_batch.shape[-1:]])\n",
    "test_label_batch_flattened = test_label_batch.reshape([-1, *test_label_batch.shape[-1:]])\n",
    "test_feature_batch_expanded = test_feature_batch_flattened if len(test_feature_batch_flattened.shape) == 3 else np.expand_dims(test_feature_batch_flattened, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model_params(params, train_X, train_Y, test_X, test_Y):\n",
    "    model = create_model(training_params)\n",
    "    history = model.fit(train_X, train_Y, validation_data=(test_X, test_Y), epochs=params['epochs'], batch_size=params['batch_size'], verbose=1, shuffle=False)\n",
    "    return model, history.history['val_loss'][-1]\n",
    "\n",
    "\n",
    "print(training_params['batch_size'])\n",
    "print(train_feature_batch.shape)\n",
    "model, result = eval_model_params(training_params, train_feature_batch, train_label_batch, test_feature_batch, test_label_batch)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_states()\n",
    "print(test_feature_batch_expanded.shape)\n",
    "\n",
    "predictions = model.predict(test_feature_batch_expanded, batch_size=BATCH_SIZE)\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "print(predictions.shape)\n",
    "print(test_label_batch_flattened.shape)\n",
    "\n",
    "print(np.round(predictions)[:-24])\n",
    "print(test_label_batch_flattened[:-24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val accuracy: \", np.sum(np.round(predictions) == test_label_batch_flattened) / predictions.size) \n",
    "print(\"Val accuracy per device:: \", np.sum(np.round(predictions) == test_label_batch_flattened, axis=0) / predictions.shape[0]) \n",
    "\n",
    "print(\"% of 1 prediction outputs\", np.sum(np.round(predictions)) / predictions.size) \n",
    "print(\"% of 1 label outputs\", np.sum(np.round(test_label_batch_flattened)) / test_label_batch_flattened.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "import json\n",
    "\n",
    "with open('params.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(training_params, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.losses\n",
    "keras.losses.weighted_loss = weighted_loss\n",
    "\n",
    "test_model_params = dict(training_params)\n",
    "test_model_params['batch_size'] = 1\n",
    "\n",
    "#test_model = load_model(\"model.h5)\n",
    "test_model = create_model(test_model_params)\n",
    "test_model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, in_file):\n",
    "    test_features, test_labels, device_list = read_and_preprocess_data(in_file, batch_size=1)\n",
    "    print(feature_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    predictions = np.concatenate(model.predict(feature_batch, batch_size=1), axis=0)\n",
    "    print(np.round(predictions))\n",
    "    print(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_model, in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_24h(model, in_file):\n",
    "    feature_batch, label_batch, device_list = read_and_preprocess_data(in_file, batch_size=1)\n",
    "    print(feature_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    predictions = np.concatenate(model.predict(feature_batch, batch_size=1), axis=0)\n",
    "    \n",
    "    all_predictions = []\n",
    "    \n",
    "    last_features = feature_batch[-1, -1]\n",
    "    last_predictions = tmp_prediction = predictions[-1]\n",
    "    \n",
    "    tmp_features = np.array(last_features)\n",
    "    tmp_features = np.concatenate([tmp_features[:2], last_predictions])\n",
    "    for i in range(24):\n",
    "        print(tmp_features)\n",
    "        #print(tmp_prediction)\n",
    "        tmp_prediction = model.predict(np.reshape(tmp_features, [1, 1, len(tmp_features)]))\n",
    "        tmp_features = np.concatenate([tmp_features[:2], tmp_prediction[0, 0]])\n",
    "        \n",
    "        # Increment time features\n",
    "        if tmp_features[1] == 23:\n",
    "            tmp_features[0] = (tmp_features[0] + 1) % 7\n",
    "        tmp_features[1] = (tmp_features[1] + 1) % 24\n",
    "        all_predictions += [tmp_prediction]\n",
    "        \n",
    "    return np.round(np.concatenate(all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_file = \"challenge/data/device_activations_medium.csv\"\n",
    "future_predictions = predict_next_24h(test_model, test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_file = \"challenge/data/device_activations_small.csv\"\n",
    "test_model.reset_states()\n",
    "feature_batch, label_batch, device_list = read_and_preprocess_data(label_file, batch_size=1)\n",
    "label_batch = label_batch.squeeze()\n",
    "print(label_batch.shape)\n",
    "print(future_predictions.shape)\n",
    "future_predictions = np.squeeze(future_predictions.astype(np.int64))\n",
    "print(future_predictions)\n",
    "print(label_batch[-24:])\n",
    "future_labels = label_batch[-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(future_predictions.shape)\n",
    "print(future_labels.shape)\n",
    "\n",
    "print(\"Test accuracy: \", np.sum(np.round(future_predictions) == future_labels) / future_labels.size) \n",
    "print(\"Test accuracy per device:: \", np.sum(np.round(future_predictions) == future_labels, axis=0) / future_labels.shape[0]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox\n",
    "n = 128\n",
    "n_sequences = 4\n",
    "sequence_length = 16\n",
    "a = np.arange(256)\n",
    "n_minibatches = n // sequence_length // n_sequences\n",
    "b = sequences = np.reshape(a, [n_sequences, n_minibatches, sequence_length, 2])\n",
    "print(b)\n",
    "print(b.shape)\n",
    "sequence_shift = n // sequence_length\n",
    "\n",
    "mini_batch_features_arr_shape = [n_minibatches * n_sequences, sequence_length, 2]\n",
    "mini_batch_features = np.zeros(mini_batch_features_arr_shape)\n",
    "mini_batch_labels_arr_shape = [n_minibatches * n_sequences, sequence_length, 2]\n",
    "mini_batch_labels = np.zeros(mini_batch_labels_arr_shape)\n",
    "for i in range(n_minibatches):\n",
    "    for j in range(n_sequences):\n",
    "        mini_batch_features[i * n_sequences + j] = b[j, i]\n",
    "        mini_batch_labels[i * n_sequences + j] = b[j, i]\n",
    "        print(i * n_sequences + j)\n",
    "        \n",
    "print(mini_batch_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes=[]\n",
    "for x in range(mini_batch_count):\n",
    "    for i in range(batch_size):\n",
    "        for j in range(sequence_length):\n",
    "            indexes += [i * sequence_shift + x * sequence_length + j]\n",
    "print(np.reshape(indexes, [mini_batch_count, batch_size, sequence_length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
