{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apram\\hodschallenge\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in c:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in c:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.12.0 in c:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from pandas) (1.16.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"challenge/data/device_activations.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n",
      "2016-07-01 04:23:32\n",
      "2016-07-01 06:52:57\n",
      "2016-07-01 06:53:00\n"
     ]
    }
   ],
   "source": [
    "previous_readings = pd.read_csv(in_file)\n",
    "print(type(previous_readings['time'][0]))\n",
    "previous_readings['time'] = pd.to_datetime(previous_readings['time'])\n",
    "device_list = sorted(previous_readings['device'].unique())\n",
    "n_devices = len(device_list)\n",
    "print(type(previous_readings['time'][0]))\n",
    "print(previous_readings.loc[0, :]['time'])\n",
    "print(previous_readings.loc[1, :]['time'])\n",
    "print(previous_readings.loc[2, :]['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points:  9045\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points: \", len(previous_readings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_time_same_hour(dt1, dt2):\n",
    "    same_until_hour = dt1.year == dt2.year and dt1.month == dt2.month and dt1.day == dt2.day and dt1.hour == dt2.hour\n",
    "    if same_until_hour:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# IDEA: ADD HOUR & DAY AS FEATURE?\n",
    "# 541 hours with positive label\n",
    "# Weekday as 7 binary inputs?\n",
    "def preallocate_features(previous_readings):\n",
    "    n_devices = previous_readings['device'].nunique()\n",
    "    print('n_devices = ', n_devices)\n",
    "    tmp_time_stamp = first_time_stamp = pd.to_datetime(previous_readings['time'][0])\n",
    "    \n",
    "    unique_hour_date_times = [first_time_stamp.replace(minute=0, second=0)]\n",
    "    \n",
    "    hour_interval_start_end = pd.date_range(first_time_stamp.replace(minute=0, second=0), \n",
    "                                            previous_readings['time'][len(previous_readings) - 1].replace(minute=0, second=0), \n",
    "                                           freq='H')\n",
    "    \n",
    "    features = pd.DataFrame(0, index=np.arange(len(hour_interval_start_end)), \n",
    "                            columns=['time', 'weekday', 'hour'] + device_list)\n",
    "    features['time'] = hour_interval_start_end\n",
    "    return features\n",
    "#print(\"Unique hours: \", len(preallocate_features(previous_readings)))\n",
    "#print(preallocate_features(previous_readings))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_devices =  7\n"
     ]
    }
   ],
   "source": [
    "def preprocess_features_and_labels(previous_readings):\n",
    "    \"\"\"\n",
    "    Generate features so that we have an array with dimensions [T, n_devices, 1]\n",
    "    T: time\n",
    "    n_devices: number of devices\n",
    "    Last dimension: number of activations per hour\n",
    "    \"\"\"\n",
    "    \n",
    "    # We preallocate to avoid many appends (append copies according to pandas docs, might become an issue/slow for large data)\n",
    "    features = preallocate_features(previous_readings)\n",
    "    labels = pd.DataFrame(0, index=np.arange(len(features) - 1), \n",
    "                            columns=device_list)\n",
    "    for index, row in previous_readings.iterrows():\n",
    "        if index == 0:\n",
    "            continue\n",
    "            \n",
    "        dt = row['time'].replace(minute=0, second=0)\n",
    "        feature_idx = features.index[features['time'] == dt]\n",
    "        # Increment device's counter at time\n",
    "        features.loc[feature_idx, row['device']] += 1\n",
    "        labels.loc[feature_idx - 1, row['device']] = 1\n",
    "            \n",
    "        features['time']\n",
    "    \n",
    "    #Second loop, can we improve here?\n",
    "    for index, row in features.iterrows():\n",
    "        features.loc[index, 'weekday'] = row['time'].weekday()\n",
    "        features.loc[index, 'hour'] = row['time'].hour\n",
    "        \n",
    "        \n",
    "    features.drop('time', axis=1, inplace=True)\n",
    "    \n",
    "    return (features, labels)\n",
    "\n",
    "features, labels = preprocess_features_and_labels(previous_readings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Features\")\n",
    "#print(features)\n",
    "\n",
    "#print(\"Labels\")\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1477, 9)\n",
      "(1477, 7)\n",
      "(1457, 9)\n",
      "(1457, 7)\n",
      "(47, 31, 9)\n",
      "(47, 31, 7)\n",
      "Train len:  32\n"
     ]
    }
   ],
   "source": [
    "def create_timeseries_batches(features, labels, length=31):\n",
    "    np_features = features.to_numpy()[:-1]\n",
    "    np_labels = labels.to_numpy()\n",
    "    \n",
    "    print(np_features.shape)\n",
    "    print(np_labels.shape)\n",
    "    \n",
    "    np_features = np_features[:length*(len(np_features)//length)]\n",
    "    np_labels = np_labels[:length*(len(np_labels)//length)]\n",
    "    \n",
    "    print(np_features.shape)\n",
    "    print(np_labels.shape)\n",
    "    \n",
    "    feature_batch = np.reshape(np_features, [-1, length, np_features.shape[1]])\n",
    "    label_batch = np.reshape(np_labels, [-1, length, np_labels.shape[1]])\n",
    "    \n",
    "    print(feature_batch.shape)\n",
    "    print(label_batch.shape)\n",
    "    \n",
    "    return feature_batch, label_batch\n",
    "\n",
    "feature_batch, label_batch = create_timeseries_batches(features, labels)\n",
    "\n",
    "train_ratio = 0.7\n",
    "train_len = int(train_ratio * len(feature_batch))\n",
    "print(\"Train len: \", train_len)\n",
    "\n",
    "train_feature_batch = feature_batch[:train_len]\n",
    "test_feature_batch = feature_batch[train_len:]\n",
    "train_label_batch = label_batch[:train_len]\n",
    "test_label_batch = label_batch[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1476, 7)\n",
      "Ratio per device:  [0.14769648 0.26287263 0.15785908 0.18631436 0.20121951 0.3299458\n",
      " 0.04471545]\n"
     ]
    }
   ],
   "source": [
    "def calc_ratio_positive_outputs_per_device(labels):\n",
    "    np_labels = labels.to_numpy()[:-1]\n",
    "    print(np_labels.shape)\n",
    "    ratio_per_device = np.sum(np_labels, axis=0) / np_labels.shape[0]\n",
    "    print(\"Ratio per device: \", ratio_per_device)\n",
    "    return np.array(ratio_per_device)\n",
    "ratio_positive_outputs_per_device = calc_ratio_positive_outputs_per_device(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own weighted loss to combat label imbalance\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    out = -(y_true * K.log(y_pred + 1e-5) / ratio_positive_outputs_per_device + (1.0 - y_true) * K.log(1.0 - y_pred + 1e-5))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "def create_model(params):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(32, batch_input_shape=(params['batch_size'], None, n_devices + 2), return_sequences=True, stateful=True))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(n_devices, activation='sigmoid'))\n",
    "    model.compile(loss=weighted_loss if params['use_weighted_loss'] else 'binary_crossentropy', optimizer=params['optimizer'])\n",
    "    return model\n",
    "\n",
    "training_params = {'optimizer': 'adam', \n",
    "                   'use_weighted_loss': USE_WEIGHTED_LOSS,\n",
    "                   'batch_size': BATCH_SIZE,\n",
    "                   'dropout': 0.0,\n",
    "                   'epochs': 50}\n",
    "\n",
    "#model = create_model(training_params)\n",
    "#model.fit(train_feature_batch, train_label_batch, epochs=training_params['epochs'], batch_size=training_params['batch_size'], verbose=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_feature_batch_flattened = test_feature_batch.reshape([-1, *test_feature_batch.shape[-1:]])\n",
    "test_label_batch_flattened = test_label_batch.reshape([-1, *test_label_batch.shape[-1:]])\n",
    "test_feature_batch_expanded = test_feature_batch_flattened if len(test_feature_batch_flattened.shape) == 3 else np.expand_dims(test_feature_batch_flattened, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 465 samples\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 1.2112 - val_loss: 1.0869\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.0265 - val_loss: 0.9607\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.8952 - val_loss: 0.8378\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.7840 - val_loss: 0.7424\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.6987 - val_loss: 0.6769\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.6355 - val_loss: 0.6254\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.5867 - val_loss: 0.5861\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.5495 - val_loss: 0.5599\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.5226 - val_loss: 0.5409\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.5018 - val_loss: 0.5264\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4840 - val_loss: 0.5160\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.4690 - val_loss: 0.5094\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.4569 - val_loss: 0.5050\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4456 - val_loss: 0.5022\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4374 - val_loss: 0.5013\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4265 - val_loss: 0.4964\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4255 - val_loss: 0.4933\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4134 - val_loss: 0.4927\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4101 - val_loss: 0.4909\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4017 - val_loss: 0.4855\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4046 - val_loss: 0.4862\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3927 - val_loss: 0.4883\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3866 - val_loss: 0.4828\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3871 - val_loss: 0.5784\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.4498 - val_loss: 0.4780\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3943 - val_loss: 0.4723\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3852 - val_loss: 0.4768\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3793 - val_loss: 0.4744\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3756 - val_loss: 0.4774\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3707 - val_loss: 0.4691\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3831 - val_loss: 0.4791\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3701 - val_loss: 0.4696\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3817 - val_loss: 0.4996\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3676 - val_loss: 0.5015\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3614 - val_loss: 0.5158\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 0.3582 - val_loss: 0.5136\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3525 - val_loss: 0.5132\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3474 - val_loss: 0.5072\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3427 - val_loss: 0.5131\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3398 - val_loss: 0.4936\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3374 - val_loss: 0.5167\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3382 - val_loss: 0.4758\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3659 - val_loss: 0.4695\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3378 - val_loss: 0.4887\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3306 - val_loss: 0.5034\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3270 - val_loss: 0.5153\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3247 - val_loss: 0.5168\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3210 - val_loss: 0.5203\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3199 - val_loss: 0.4949\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.3168 - val_loss: 0.5162\n",
      "0.5162368283456852\n"
     ]
    }
   ],
   "source": [
    "def eval_model_params(params, train_X, train_Y, test_X, test_Y):\n",
    "    model = create_model(training_params)\n",
    "    history = model.fit(train_X, train_Y, validation_data=(test_X, np.expand_dims(test_Y, 1)), epochs=params['epochs'], batch_size=params['batch_size'], verbose=1, shuffle=False)\n",
    "    predictions = model.predict(test_X, batch_size=1)\n",
    "    return model, history.history['val_loss'][-1]\n",
    "\n",
    "model, result = eval_model_params(training_params, train_feature_batch, train_label_batch, test_feature_batch_expanded,test_label_batch_flattened)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 1, 9)\n",
      "(465, 7)\n",
      "(465, 7)\n"
     ]
    }
   ],
   "source": [
    "print(test_feature_batch_expanded.shape)\n",
    "predictions = model.predict(test_feature_batch_expanded, batch_size=1)[:, 0, :]\n",
    "#print(np.round(predictions, 1))\n",
    "\n",
    "print(predictions.shape)\n",
    "print(test_label_batch_flattened.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8740399385560675\n",
      "Training accuracy per device::  [0.90967742 0.92688172 0.84516129 0.80860215 0.85591398 0.92043011\n",
      " 0.8516129 ]\n",
      "% of 1 prediction outputs 0.25099846390168973\n",
      "% of 1 label outputs 0.17357910906298002\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuracy: \", np.sum(np.round(predictions) == test_label_batch_flattened) / predictions.size) \n",
    "print(\"Training accuracy per device:: \", np.sum(np.round(predictions) == test_label_batch_flattened, axis=0) / predictions.shape[0]) \n",
    "\n",
    "print(\"% of 1 prediction outputs\", np.sum(np.round(predictions)) / predictions.size) \n",
    "print(\"% of 1 label outputs\", np.sum(np.round(test_label_batch_flattened)) / test_label_batch_flattened.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(465, 1, 9)\n",
      "(1, 465, 7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 465 samples",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-31fb51532fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_feature_batch_expanded\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_label_batch_flattened\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_feature_batch_flattened\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_label_batch_flattened\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1100\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\apram\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    820\u001b[0m                                  \u001b[1;34m'a number of samples that can be '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m                                  \u001b[1;34m'divided by the batch size. Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m                                  str(x[0].shape[0]) + ' samples')\n\u001b[0m\u001b[0;32m    823\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: In a stateful network, you should only pass inputs with a number of samples that can be divided by the batch size. Found: 465 samples"
     ]
    }
   ],
   "source": [
    "print(test_feature_batch_expanded.shape)\n",
    "print(np.expand_dims(test_label_batch_flattened, 0).shape)\n",
    "model.evaluate(np.expand_dims(test_feature_batch_flattened, 1), np.expand_dims(test_label_batch_flattened, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
