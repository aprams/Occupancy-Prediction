{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy Timeseries Forecast\n",
    "## Predict occupancies for future hours from a known history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "from preprocess import read_and_preprocess_data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.regularizers import l2\n",
    "from keras import objectives\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file = \"challenge/data/device_activations_train.csv\"\n",
    "val_in_file = \"challenge/data/device_activations_val.csv\"\n",
    "test_in_file = \"challenge/data/device_activations_test.csv\"\n",
    "device_list=['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7']\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load & preprocess data\n",
    "Load data from csv files, expects different files for training, validation and test (to be specified in the cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hours in data:  1014\n",
      "File challenge/data/device_activations_train.csv has 1013 timesteps (hours) until now\n",
      "initial features shape:  (1013, 16)\n",
      "Full sequence length:  525\n",
      "Sequence 0 has start index 0 and end index 525\n",
      "(525, 16)\n",
      "Sequence 1 has start index 30 and end index 555\n",
      "(525, 16)\n",
      "Sequence 2 has start index 60 and end index 585\n",
      "(525, 16)\n",
      "Sequence 3 has start index 90 and end index 615\n",
      "(525, 16)\n",
      "Sequence 4 has start index 120 and end index 645\n",
      "(525, 16)\n",
      "Sequence 5 has start index 150 and end index 675\n",
      "(525, 16)\n",
      "Sequence 6 has start index 180 and end index 705\n",
      "(525, 16)\n",
      "Sequence 7 has start index 210 and end index 735\n",
      "(525, 16)\n",
      "Sequence 8 has start index 240 and end index 765\n",
      "(525, 16)\n",
      "Sequence 9 has start index 270 and end index 795\n",
      "(525, 16)\n",
      "Sequence 10 has start index 300 and end index 825\n",
      "(525, 16)\n",
      "Sequence 11 has start index 330 and end index 855\n",
      "(525, 16)\n",
      "Sequence 12 has start index 360 and end index 885\n",
      "(525, 16)\n",
      "Sequence 13 has start index 390 and end index 915\n",
      "(525, 16)\n",
      "Sequence 14 has start index 420 and end index 945\n",
      "(525, 16)\n",
      "Sequence 15 has start index 450 and end index 975\n",
      "(525, 16)\n",
      "Features sequences shape:  (16, 21, 25, 16)\n",
      "Labels sequences shape:  (16, 21, 25, 7)\n",
      "[[ 4.          4.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.02366864  0.        ]\n",
      " [ 4.          5.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.          6.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          0.28402367  0.02366864\n",
      "   0.          0.          0.1183432   0.        ]\n",
      " [ 4.          7.          0.          1.          0.          1.\n",
      "   0.          1.          0.          0.1183432   1.06508876  0.07100592\n",
      "   0.09467456  0.14201183  0.52071006  0.        ]\n",
      " [ 4.          8.          0.          1.          1.          1.\n",
      "   1.          1.          0.          0.16568047  1.11242604  0.1183432\n",
      "   0.21301775  0.61538462  0.63905325  0.        ]\n",
      " [ 4.          9.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.1183432   0.87573964  0.14201183\n",
      "   0.35502959  0.49704142  0.56804734  0.        ]\n",
      " [ 4.         10.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.          0.5443787   0.16568047\n",
      "   0.14201183  0.4260355   0.33136095  0.        ]\n",
      " [ 4.         11.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.04733728  0.56804734  0.04733728\n",
      "   0.28402367  0.33136095  0.4260355   0.        ]\n",
      " [ 4.         12.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.09467456  0.94674556  0.14201183\n",
      "   0.1183432   0.09467456  0.47337278  0.        ]\n",
      " [ 4.         13.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.16568047  1.13609467  0.33136095\n",
      "   0.30769231  0.4260355   0.49704142  0.        ]\n",
      " [ 4.         14.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.07100592  0.94674556  0.23668639\n",
      "   0.26035503  0.5443787   0.61538462  0.02366864]\n",
      " [ 4.         15.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.          1.11242604  0.23668639\n",
      "   0.14201183  0.23668639  0.47337278  0.02366864]\n",
      " [ 4.         16.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          1.01775148  0.\n",
      "   0.          0.          0.23668639  0.02366864]\n",
      " [ 4.         17.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.23668639  0.\n",
      "   0.          0.02366864  0.18934911  0.        ]\n",
      " [ 4.         18.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         19.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         21.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         22.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 4.         23.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.02366864  0.        ]\n",
      " [ 5.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.          2.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.          3.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.          4.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[[ 5.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         11.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         12.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         13.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         14.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         15.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         16.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         17.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         18.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         19.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         21.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         22.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 5.         23.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          2.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.02366864  0.        ]\n",
      " [ 6.          3.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.1183432   0.02366864]\n",
      " [ 6.          4.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.04733728  0.        ]\n",
      " [ 6.          5.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          6.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          7.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          8.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.          9.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 6.         10.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[[ 1.         18.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.16568047\n",
      "   0.          0.          0.1183432   0.        ]\n",
      " [ 1.         19.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.14201183\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.         20.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.23668639\n",
      "   0.          0.          0.04733728  0.        ]\n",
      " [ 1.         21.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.         22.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 1.         23.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          2.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          3.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          4.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          5.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 2.          6.          0.          1.          0.          0.\n",
      "   0.          0.          0.          0.          0.26035503  0.\n",
      "   0.          0.          0.23668639  0.        ]\n",
      " [ 2.          7.          1.          1.          0.          0.\n",
      "   0.          0.          0.          0.28402367  1.32544379  0.07100592\n",
      "   0.09467456  0.09467456  0.56804734  0.        ]\n",
      " [ 2.          8.          1.          1.          1.          0.\n",
      "   0.          0.          0.          0.40236686  0.92307692  0.1183432\n",
      "   0.35502959  0.59171598  0.63905325  0.09467456]\n",
      " [ 2.          9.          1.          1.          1.          0.\n",
      "   0.          0.          0.          0.33136095  0.56804734  0.21301775\n",
      "   0.56804734  0.5443787   0.73372781  0.14201183]\n",
      " [ 2.         10.          0.          1.          1.          0.\n",
      "   0.          0.          0.          0.30769231  0.30769231  0.1183432\n",
      "   0.4260355   0.5443787   0.61538462  0.07100592]\n",
      " [ 2.         11.          0.          1.          0.          0.\n",
      "   1.          1.          0.          0.1183432   0.49704142  0.37869822\n",
      "   0.37869822  0.59171598  0.61538462  0.18934911]\n",
      " [ 2.         12.          1.          1.          0.          1.\n",
      "   1.          1.          0.          0.30769231  0.61538462  0.18934911\n",
      "   0.23668639  0.37869822  0.82840237  0.16568047]\n",
      " [ 2.         13.          1.          1.          0.          0.\n",
      "   0.          1.          0.          0.4260355   0.85207101  0.21301775\n",
      "   0.49704142  0.56804734  0.71005917  0.14201183]\n",
      " [ 2.         14.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.40236686  0.99408284  0.26035503\n",
      "   0.52071006  0.44970414  0.80473373  0.16568047]\n",
      " [ 2.         15.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.35502959  0.92307692  0.4260355\n",
      "   0.5443787   0.80473373  0.66272189  0.1183432 ]\n",
      " [ 2.         16.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.30769231  0.85207101  0.14201183\n",
      "   0.35502959  0.82840237  0.56804734  0.07100592]\n",
      " [ 2.         17.          0.          0.          0.          1.\n",
      "   1.          1.          1.          0.          0.28402367  0.09467456\n",
      "   0.40236686  0.63905325  0.61538462  0.02366864]\n",
      " [ 2.         18.          0.          0.          0.          1.\n",
      "   1.          1.          0.          0.          0.          0.04733728\n",
      "   0.1183432   0.14201183  0.16568047  0.        ]]\n",
      "Feature batch:  (336, 25, 16)\n",
      "Label batch:  (336, 25, 7)\n"
     ]
    }
   ],
   "source": [
    "feature_batch, label_batch, device_list, mean_occupancies = read_and_preprocess_data(in_file, batch_size=BATCH_SIZE, device_list=device_list, sequence_start_shift=30)\n",
    "print(\"Feature batch shape: \", feature_batch.shape)\n",
    "print(\"Label batch shape: \", label_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean occupancy visualization per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_occupancy_per_hour = mean_occupancies.to_numpy().reshape([168, 7]).sum(axis=1) / 7\n",
    "labels = range(len(mean_occupancy_per_hour))\n",
    "plt.bar(labels, mean_occupancy_per_hour, color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hours in data:  288\n",
      "File challenge/data/device_activations_val.csv has 287 timesteps (hours) until now\n",
      "Hours in data:  176\n",
      "File challenge/data/device_activations_test.csv has 175 timesteps (hours) until now\n"
     ]
    }
   ],
   "source": [
    "val_features, val_labels, _, _ = read_and_preprocess_data(val_in_file, batch_size=1, device_list=device_list)\n",
    "test_features, test_labels, _, _ = read_and_preprocess_data(test_in_file, batch_size=1, device_list=device_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate ratio of positive to negative labels per device to adjust loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive outputs per device:  [0.0777381  0.2475     0.16202381 0.24214286 0.23559524 0.34678571\n",
      " 0.03559524]\n"
     ]
    }
   ],
   "source": [
    "def calc_ratio_positive_outputs_per_device(labels):\n",
    "    ratio_per_device = np.sum(labels, axis=0) / labels.shape[0]\n",
    "    print(\"Ratio of positive to negative outputs per device: \", ratio_per_device)\n",
    "    return np.array(ratio_per_device)\n",
    "ratio_positive_outputs_per_device = calc_ratio_positive_outputs_per_device(label_batch.reshape([-1, label_batch.shape[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_WEIGHTED_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted loss: Scales up loss for positive labels per-device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own weighted loss to combat label imbalance\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    out = -(y_true * K.log(y_pred + 1e-5) / ratio_positive_outputs_per_device + (1.0 - y_true) * K.log(1.0 - y_pred + 1e-5))\n",
    "    return K.mean(out, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(params):\n",
    "    \"\"\"\n",
    "    Creates an LSTM-based model given the parameters: 'lr', 'do', 'reg', 'lstm_units', 'n_outputs', 'n_features', 'use_weighted_loss'.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['lstm_units'], batch_input_shape=(params['batch_size'], None, params['n_features']), return_sequences=True, stateful=True, kernel_regularizer=l2(params['reg'])))\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    model.add(Dense(params['n_outputs'], activation='sigmoid'))\n",
    "    \n",
    "    adam = optimizers.Adam(lr=params['lr'])\n",
    "    model.compile(loss=weighted_loss if params['use_weighted_loss'] else 'binary_crossentropy', optimizer=adam)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_24h(model, features, labels):\n",
    "    \"\"\"\n",
    "    Predicts a model's output for given features and the following 24 hours, reusing predictions as inputs\n",
    "    \"\"\"\n",
    "    predictions = np.squeeze(model.predict(np.expand_dims(features, 0), batch_size=1))  # (n_timesteps, n_outputs)\n",
    "\n",
    "    all_predictions = []\n",
    "\n",
    "    last_features = np.squeeze(features)[-1]\n",
    "    last_predictions = tmp_prediction = predictions[-1]\n",
    "\n",
    "    tmp_features = np.array(last_features)\n",
    "    tmp_mean_occupancies = [mean_occupancies.loc[(tmp_features[0] * 24 + tmp_features[1], 'device_' + str(i + 1)), \n",
    "                                                 'mean_occupancy'] for i in range(len(device_list))]\n",
    "\n",
    "    tmp_features = np.concatenate([tmp_features[:2], last_predictions, tmp_mean_occupancies])\n",
    "    for i in range(24):\n",
    "        # print(tmp_prediction)\n",
    "        tmp_prediction = np.round(model.predict(np.reshape(tmp_features, [1, 1, len(tmp_features)])))\n",
    "        \n",
    "        tmp_mean_occupancies = [mean_occupancies.loc[(tmp_features[0] * 24 + tmp_features[1], 'device_' + str(j + 1)), \n",
    "                                                     'mean_occupancy'] for j in range(len(device_list))]\n",
    "        tmp_features = np.concatenate([tmp_features[:2], tmp_prediction[0, 0], tmp_mean_occupancies])\n",
    "        \n",
    "        # Increment time features\n",
    "        if tmp_features[1] == 23:\n",
    "            tmp_features[0] = (tmp_features[0] + 1) % 7\n",
    "        tmp_features[1] = (tmp_features[1] + 1) % 24\n",
    "        all_predictions += [tmp_prediction]\n",
    "\n",
    "    return np.concatenate(all_predictions)\n",
    "\n",
    "def calc_accuracy(model, params, test_X, test_Y):\n",
    "    \"\"\"\n",
    "    Calculates accuracy of a model on a test (or validation) set. Generates sub-sequences of the test set to test on.\n",
    "    The model gets 48+i hours as inputs before having to predict the next 24 hours (i in range(48, n - 24) with \n",
    "    n = hours in test set). This gives more \"effective\" test set size than splitting the test set in non-overlapping\n",
    "    sequences. Depending on the real world scenario, the assumption of at least 48 hours of input might be relaxed/\n",
    "    strictened.\n",
    "    \"\"\"\n",
    "    # Hack around Keras batch size restriction (to have same for training/test)\n",
    "    model.save('tmp_model.h5')\n",
    "    test_params = dict(params)\n",
    "    test_params['batch_size'] = 1\n",
    "    test_model = create_model(test_params)\n",
    "    test_model.load_weights('tmp_model.h5')\n",
    "    os.remove('tmp_model.h5')\n",
    "    n = test_X.shape[1]\n",
    "    acc_accumulated = 0.0\n",
    "    for i in range(48, n - 24):\n",
    "        predictions = np.squeeze(predict_24h(test_model, test_X[0, :i], test_Y[0, :i]))\n",
    "        true_labels = test_Y[0, i:i+24]\n",
    "        acc = np.sum(np.round(predictions) == true_labels) / predictions.size\n",
    "        acc_accumulated += acc\n",
    "        #print(\"Val accuracy: \", np.sum(np.round(predictions) == true_labels) / predictions.size) \n",
    "        #print(\"Val accuracy per device:: \", np.sum(np.round(predictions) == true_labels, axis=0) / predictions.shape[0]) \n",
    "\n",
    "    return acc_accumulated / (n-26)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model_with_params(params, train_X, train_Y):\n",
    "    model = create_model(params)\n",
    "    history = model.fit(train_X, train_Y, epochs=params['epochs'], batch_size=params['batch_size'], verbose=0, shuffle=False)\n",
    "    return model\n",
    "\n",
    "def eval_model_params(params, train_X, train_Y, val_X, val_Y):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a model on a given validation set by calculating its accuracy on a 24h prediction basis as \n",
    "    described in calc_accuracy().\n",
    "    \"\"\"\n",
    "    model = train_model_with_params(params, feature_batch, label_batch)\n",
    "    val_acc = calc_accuracy(model, params, val_features, val_labels)\n",
    "    return model, val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter (Grid) Search\n",
    "Usually I use the BayesianOptimization framework to efficiently calculate and evaluate hyperparameter spaces as GS\n",
    "gets out of hand quickly in terms of computational efficiency. In this case I simply ran out of time for that,\n",
    "so GS had to suffice. RS would have been an option, but I'm not too much of a fan and the training times in this \n",
    "project were \"ok\", so GS was my choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp result:  0.6270753512132818 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 16, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6290366721401206 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 16, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6117268746579098 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 16, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6551952198503929 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6617405582922833 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6330049261083747 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6619002006933052 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 64, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6614896916621062 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 64, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6532795110381329 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 64, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6714331326400284 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6758118956394813 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6742382776865542 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6774083196497 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 256, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6613528553183728 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 256, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6830414158000365 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 256, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6735769020251777 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 512, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6658456486042694 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 512, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6746487867177534 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 512, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6519111476008032 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 16, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.5915891260718849 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 16, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6266192300675059 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 16, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6495165115854764 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6461868272213099 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6370644043057833 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 32, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6488779419813909 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 64, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.643336070060208 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 64, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6507024265644958 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 64, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6737137383689101 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6702700237182994 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tmp result:  0.6739189928845105 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n",
      "Tmp result:  0.6773399014778334 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 256, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.0}\n",
      "Tmp result:  0.6919813902572524 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 256, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
      "Tmp result:  0.6799169859514682 {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.2, 'epochs': 250, 'n_outputs': 7, 'n_features': 16, 'lstm_units': 256, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-70a76ee5aff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    \u001b[0;34m'devices'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdevice_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                    'reg': reg}\n\u001b[0;32m---> 18\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                     \u001b[0mgs_results\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tmp result: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-79cee8cd151b>\u001b[0m in \u001b[0;36meval_model_params\u001b[0;34m(params, train_X, train_Y, val_X, val_Y)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0meval_model_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-79cee8cd151b>\u001b[0m in \u001b[0;36mtrain_model_with_params\u001b[0;34m(params, train_X, train_Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hodschallenge/.venv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gs_results = []\n",
    "for do in [0.0, 0.2, 0.5]:\n",
    "    for units in [16, 32, 64, 128, 256, 512]:\n",
    "        for reg in [0.0, 0.01, 0.1]:\n",
    "            for n_epochs in [250]:\n",
    "                for lr in [1e-3, 1e-4, 1e-5]:\n",
    "                    K.clear_session()\n",
    "                    tmp_params = {'lr': lr, \n",
    "                                   'use_weighted_loss': USE_WEIGHTED_LOSS,\n",
    "                                   'batch_size': BATCH_SIZE,\n",
    "                                   'dropout': do,\n",
    "                                   'epochs': n_epochs,\n",
    "                                   'n_outputs': len(device_list),\n",
    "                                   'n_features': feature_batch.shape[-1],\n",
    "                                   'lstm_units': units,\n",
    "                                   'devices': device_list,\n",
    "                                   'reg': reg}\n",
    "                    model, val_acc = eval_model_params(tmp_params, feature_batch, label_batch, val_features, val_labels)\n",
    "                    gs_results += [(val_acc, tmp_params)]\n",
    "                    print(\"Tmp result: \", val_acc, tmp_params)\n",
    "                \n",
    "for x in sorted(gs_results, key=lambda x: x[0], reverse=True):\n",
    "    print(\"Acc: {0}, params: {1}\".format(x[0], x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with results from GS to evaluate\n",
    "Given the parameters from the grid search, evaluate also on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc for model 0.6212370005473451\n"
     ]
    }
   ],
   "source": [
    "#Acc: 0.6819011129355954, params: {'lr': 0.001, 'use_weighted_loss': True, 'batch_size': 16, 'dropout': 0.0, 'epochs': 250, 'n_outputs': 7, 'n_features': 9, 'lstm_units': 128, 'devices': ['device_1', 'device_2', 'device_3', 'device_4', 'device_5', 'device_6', 'device_7'], 'reg': 0.01}\n",
    "\n",
    "best_params = sorted(gs_results, key=lambda x: x[0], reverse=True)[0][1]\n",
    "\n",
    "#best_params = {'lr': 0.0001, \n",
    "#               'use_weighted_loss': True,\n",
    "#               'batch_size': BATCH_SIZE,\n",
    "#               'dropout': 0.0,\n",
    "#               'epochs': 250,\n",
    "#               'n_outputs': len(device_list),\n",
    "#               'n_features': feature_batch.shape[-1],\n",
    "#               'lstm_units': 128,\n",
    "#               'devices': device_list,\n",
    "#               'reg': 0.01}\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model, val_acc = eval_model_params(best_params, feature_batch, label_batch, val_features, val_labels)\n",
    "print(\"Val acc for model\", val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val acc:  0.6212370005473451\n",
      "Test acc:  0.6133748801534037\n"
     ]
    }
   ],
   "source": [
    "test_acc = calc_accuracy(model, best_params, test_features, test_labels)\n",
    "print(\"Test acc: \", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving models and parameters\n",
    "Save the trained model to be reused in the inference/server script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "import json\n",
    "\n",
    "with open('params.json', 'w', encoding='utf-8') as fp:\n",
    "    json.dump(training_params, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
